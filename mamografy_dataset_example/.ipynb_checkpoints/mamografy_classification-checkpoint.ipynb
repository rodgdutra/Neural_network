{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rodrigo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "data_csv = \"mamografy.csv\"\n",
    "dataframe = pd.read_csv(data_csv, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# Normalize the dataset\n",
    "start = 0\n",
    "end = 5\n",
    "norm_dataset = list()\n",
    "\n",
    "for i in range(start, end):\n",
    "    norm_col = dataset[:, i] / np.amax(dataset[:, i])\n",
    "    norm_dataset.append(norm_col)\n",
    "\n",
    "for i in range(end, end + 2):\n",
    "    col = dataset[:, i]\n",
    "    norm_dataset.append(col)\n",
    "\n",
    "norm_dataset = pd.DataFrame(norm_dataset)\n",
    "norm_dataset = norm_dataset.transpose()\n",
    "norm_dataset = norm_dataset.values\n",
    "\n",
    "dataset = norm_dataset\n",
    "\n",
    "x = dataset[:, 0:5].astype(float)\n",
    "y = dataset[:, 5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data\n",
    "\n",
    "seed = 9\n",
    "# Split data set into train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.15, random_state=seed\n",
    ")\n",
    "# Split train into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train, y_train, test_size=0.175, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rodrigo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 2s 3ms/step - loss: 0.8009 - acc: 0.5154 - val_loss: 0.5665 - val_acc: 0.5317\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 55us/step - loss: 0.4646 - acc: 0.5154 - val_loss: 0.3346 - val_acc: 0.5317\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.2946 - acc: 0.5154 - val_loss: 0.2405 - val_acc: 0.5317\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 46us/step - loss: 0.2316 - acc: 0.5394 - val_loss: 0.2195 - val_acc: 0.7222\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.2201 - acc: 0.7860 - val_loss: 0.2180 - val_acc: 0.8095\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.2181 - acc: 0.7945 - val_loss: 0.2170 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 48us/step - loss: 0.2168 - acc: 0.7979 - val_loss: 0.2148 - val_acc: 0.8254\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.2149 - acc: 0.8082 - val_loss: 0.2125 - val_acc: 0.8016\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.2135 - acc: 0.8305 - val_loss: 0.2104 - val_acc: 0.8175\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 49us/step - loss: 0.2119 - acc: 0.8271 - val_loss: 0.2086 - val_acc: 0.8175\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 44us/step - loss: 0.2100 - acc: 0.8271 - val_loss: 0.2067 - val_acc: 0.8175\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.2086 - acc: 0.8305 - val_loss: 0.2047 - val_acc: 0.8254\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.2069 - acc: 0.8219 - val_loss: 0.2029 - val_acc: 0.8254\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 52us/step - loss: 0.2052 - acc: 0.8271 - val_loss: 0.2008 - val_acc: 0.8333\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.2034 - acc: 0.8236 - val_loss: 0.1988 - val_acc: 0.8333\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.2017 - acc: 0.8236 - val_loss: 0.1968 - val_acc: 0.8254\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.2000 - acc: 0.8271 - val_loss: 0.1949 - val_acc: 0.8254\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1983 - acc: 0.8202 - val_loss: 0.1930 - val_acc: 0.8254\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 49us/step - loss: 0.1966 - acc: 0.8168 - val_loss: 0.1910 - val_acc: 0.8254\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1950 - acc: 0.8151 - val_loss: 0.1891 - val_acc: 0.8254\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 52us/step - loss: 0.1935 - acc: 0.8219 - val_loss: 0.1873 - val_acc: 0.8175\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 53us/step - loss: 0.1918 - acc: 0.8185 - val_loss: 0.1853 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1901 - acc: 0.8168 - val_loss: 0.1835 - val_acc: 0.8175\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1887 - acc: 0.8219 - val_loss: 0.1818 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1871 - acc: 0.8202 - val_loss: 0.1800 - val_acc: 0.8175\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 55us/step - loss: 0.1856 - acc: 0.8168 - val_loss: 0.1783 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1841 - acc: 0.8151 - val_loss: 0.1765 - val_acc: 0.8254\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1827 - acc: 0.8219 - val_loss: 0.1748 - val_acc: 0.8254\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1810 - acc: 0.8185 - val_loss: 0.1732 - val_acc: 0.8254\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1802 - acc: 0.8014 - val_loss: 0.1720 - val_acc: 0.8254\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 43us/step - loss: 0.1784 - acc: 0.8048 - val_loss: 0.1701 - val_acc: 0.8254\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 47us/step - loss: 0.1771 - acc: 0.8185 - val_loss: 0.1686 - val_acc: 0.8254\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 45us/step - loss: 0.1758 - acc: 0.8168 - val_loss: 0.1671 - val_acc: 0.8254\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1746 - acc: 0.8116 - val_loss: 0.1657 - val_acc: 0.8254\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 48us/step - loss: 0.1733 - acc: 0.8116 - val_loss: 0.1644 - val_acc: 0.8254\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1721 - acc: 0.8031 - val_loss: 0.1630 - val_acc: 0.8254\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 44us/step - loss: 0.1710 - acc: 0.8099 - val_loss: 0.1616 - val_acc: 0.8254\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 40us/step - loss: 0.1697 - acc: 0.8116 - val_loss: 0.1603 - val_acc: 0.8254\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 48us/step - loss: 0.1686 - acc: 0.8082 - val_loss: 0.1592 - val_acc: 0.8254\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 43us/step - loss: 0.1676 - acc: 0.8065 - val_loss: 0.1579 - val_acc: 0.8254\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 55us/step - loss: 0.1666 - acc: 0.8065 - val_loss: 0.1567 - val_acc: 0.8254\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 43us/step - loss: 0.1655 - acc: 0.8099 - val_loss: 0.1556 - val_acc: 0.8254\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1646 - acc: 0.8082 - val_loss: 0.1546 - val_acc: 0.8254\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.1639 - acc: 0.8065 - val_loss: 0.1536 - val_acc: 0.8254\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 52us/step - loss: 0.1630 - acc: 0.8048 - val_loss: 0.1526 - val_acc: 0.8254\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.1619 - acc: 0.8082 - val_loss: 0.1516 - val_acc: 0.8254\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 45us/step - loss: 0.1611 - acc: 0.8048 - val_loss: 0.1507 - val_acc: 0.8254\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1603 - acc: 0.8065 - val_loss: 0.1498 - val_acc: 0.8254\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1594 - acc: 0.8116 - val_loss: 0.1490 - val_acc: 0.8254\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1588 - acc: 0.8116 - val_loss: 0.1481 - val_acc: 0.8254\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1583 - acc: 0.8065 - val_loss: 0.1474 - val_acc: 0.8254\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 35us/step - loss: 0.1573 - acc: 0.8065 - val_loss: 0.1465 - val_acc: 0.8254\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1566 - acc: 0.8082 - val_loss: 0.1458 - val_acc: 0.8254\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1560 - acc: 0.8116 - val_loss: 0.1451 - val_acc: 0.8254\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1554 - acc: 0.8116 - val_loss: 0.1445 - val_acc: 0.8254\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1547 - acc: 0.8099 - val_loss: 0.1438 - val_acc: 0.8254\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1543 - acc: 0.8014 - val_loss: 0.1432 - val_acc: 0.8254\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1540 - acc: 0.8116 - val_loss: 0.1428 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1533 - acc: 0.8065 - val_loss: 0.1421 - val_acc: 0.8254\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1526 - acc: 0.8031 - val_loss: 0.1415 - val_acc: 0.8254\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1523 - acc: 0.8116 - val_loss: 0.1410 - val_acc: 0.8254\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1517 - acc: 0.8099 - val_loss: 0.1405 - val_acc: 0.8254\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1513 - acc: 0.8082 - val_loss: 0.1401 - val_acc: 0.8254\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1509 - acc: 0.8099 - val_loss: 0.1398 - val_acc: 0.8175\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 44us/step - loss: 0.1504 - acc: 0.8116 - val_loss: 0.1393 - val_acc: 0.8254\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 47us/step - loss: 0.1503 - acc: 0.8048 - val_loss: 0.1389 - val_acc: 0.8254\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 39us/step - loss: 0.1497 - acc: 0.7997 - val_loss: 0.1385 - val_acc: 0.8254\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 47us/step - loss: 0.1495 - acc: 0.8082 - val_loss: 0.1384 - val_acc: 0.8175\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 41us/step - loss: 0.1491 - acc: 0.8099 - val_loss: 0.1378 - val_acc: 0.8254\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1489 - acc: 0.7962 - val_loss: 0.1375 - val_acc: 0.8254\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1486 - acc: 0.8099 - val_loss: 0.1374 - val_acc: 0.8175\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1484 - acc: 0.8116 - val_loss: 0.1368 - val_acc: 0.8254\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1479 - acc: 0.8014 - val_loss: 0.1366 - val_acc: 0.8254\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1478 - acc: 0.8116 - val_loss: 0.1367 - val_acc: 0.8175\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1474 - acc: 0.8116 - val_loss: 0.1361 - val_acc: 0.8254\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 38us/step - loss: 0.1470 - acc: 0.8082 - val_loss: 0.1358 - val_acc: 0.8254\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1471 - acc: 0.7997 - val_loss: 0.1356 - val_acc: 0.8254\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 55us/step - loss: 0.1465 - acc: 0.8082 - val_loss: 0.1355 - val_acc: 0.8254\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 41us/step - loss: 0.1464 - acc: 0.8116 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1464 - acc: 0.8134 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1463 - acc: 0.8031 - val_loss: 0.1348 - val_acc: 0.8254\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1458 - acc: 0.8031 - val_loss: 0.1346 - val_acc: 0.8254\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1456 - acc: 0.8065 - val_loss: 0.1345 - val_acc: 0.8254\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1454 - acc: 0.8065 - val_loss: 0.1343 - val_acc: 0.8254\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1453 - acc: 0.8048 - val_loss: 0.1341 - val_acc: 0.8254\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1452 - acc: 0.8099 - val_loss: 0.1343 - val_acc: 0.8175\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1449 - acc: 0.8099 - val_loss: 0.1339 - val_acc: 0.8254\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 41us/step - loss: 0.1452 - acc: 0.7962 - val_loss: 0.1337 - val_acc: 0.8254\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 44us/step - loss: 0.1445 - acc: 0.8099 - val_loss: 0.1339 - val_acc: 0.8175\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1447 - acc: 0.8099 - val_loss: 0.1337 - val_acc: 0.8175\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1445 - acc: 0.8082 - val_loss: 0.1334 - val_acc: 0.8254\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1442 - acc: 0.8116 - val_loss: 0.1334 - val_acc: 0.8254\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1440 - acc: 0.8099 - val_loss: 0.1332 - val_acc: 0.8254\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1439 - acc: 0.8082 - val_loss: 0.1331 - val_acc: 0.8254\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1438 - acc: 0.8099 - val_loss: 0.1330 - val_acc: 0.8254\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1437 - acc: 0.8031 - val_loss: 0.1329 - val_acc: 0.8254\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1436 - acc: 0.8065 - val_loss: 0.1330 - val_acc: 0.8254\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1435 - acc: 0.8099 - val_loss: 0.1329 - val_acc: 0.8254\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1434 - acc: 0.8099 - val_loss: 0.1327 - val_acc: 0.8254\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1433 - acc: 0.8099 - val_loss: 0.1326 - val_acc: 0.8254\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1433 - acc: 0.8048 - val_loss: 0.1325 - val_acc: 0.8254\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1434 - acc: 0.8031 - val_loss: 0.1324 - val_acc: 0.8254\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1435 - acc: 0.8099 - val_loss: 0.1331 - val_acc: 0.8175\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1430 - acc: 0.8116 - val_loss: 0.1323 - val_acc: 0.8254\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1430 - acc: 0.8014 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1429 - acc: 0.8065 - val_loss: 0.1325 - val_acc: 0.8254\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1426 - acc: 0.8082 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1426 - acc: 0.8099 - val_loss: 0.1321 - val_acc: 0.8254\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 41us/step - loss: 0.1427 - acc: 0.8134 - val_loss: 0.1321 - val_acc: 0.8254\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 52us/step - loss: 0.1427 - acc: 0.8031 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 54us/step - loss: 0.1426 - acc: 0.8082 - val_loss: 0.1324 - val_acc: 0.8175\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 52us/step - loss: 0.1422 - acc: 0.8099 - val_loss: 0.1320 - val_acc: 0.8254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 39us/step - loss: 0.1422 - acc: 0.8048 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1421 - acc: 0.8065 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1420 - acc: 0.8116 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 33us/step - loss: 0.1419 - acc: 0.8082 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1419 - acc: 0.8082 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1417 - acc: 0.8065 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 34us/step - loss: 0.1417 - acc: 0.8065 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1423 - acc: 0.8099 - val_loss: 0.1321 - val_acc: 0.8254\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1415 - acc: 0.8099 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1416 - acc: 0.8031 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1415 - acc: 0.8082 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 37us/step - loss: 0.1415 - acc: 0.8134 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1412 - acc: 0.8082 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 35us/step - loss: 0.1412 - acc: 0.8065 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 44us/step - loss: 0.1412 - acc: 0.8082 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.1410 - acc: 0.8099 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.1413 - acc: 0.8134 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 49us/step - loss: 0.1411 - acc: 0.8116 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 131/200\n",
      "584/584 [==============================] - 0s 49us/step - loss: 0.1412 - acc: 0.8065 - val_loss: 0.1312 - val_acc: 0.8254\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1411 - acc: 0.8116 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 38us/step - loss: 0.1408 - acc: 0.8099 - val_loss: 0.1312 - val_acc: 0.8254\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 40us/step - loss: 0.1408 - acc: 0.8082 - val_loss: 0.1312 - val_acc: 0.8254\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 42us/step - loss: 0.1408 - acc: 0.8065 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 46us/step - loss: 0.1406 - acc: 0.8116 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 36us/step - loss: 0.1406 - acc: 0.8134 - val_loss: 0.1312 - val_acc: 0.8254\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 47us/step - loss: 0.1409 - acc: 0.8065 - val_loss: 0.1310 - val_acc: 0.8254\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 38us/step - loss: 0.1404 - acc: 0.8134 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the NN\n",
    "lr = 0.001  # learning rate\n",
    "lr_decay = 0.0005# learning rate decay\n",
    "n_mini_batch = 100  # mini-batch length\n",
    "activation_fcn = \"sigmoid\"\n",
    "optimizer = Adam(lr=lr, decay=lr_decay)\n",
    "input_dim = x.shape[1]\n",
    "h_n = 30\n",
    "model = Sequential()\n",
    "model.add(Dense(h_n, input_dim=input_dim, activation=activation_fcn))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train and validate the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train[:, 0],\n",
    "    validation_data=(x_val, y_val[:, 0]),\n",
    "    epochs=200,\n",
    "    batch_size=50,\n",
    "    # verbose=0,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", mode=\"min\", min_delta=0.001, patience=20, verbose=1\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Plot the validation and train loss\n",
    "plt.figure(figsize=(8, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(np.sqrt(history.history[\"loss\"]))\n",
    "plt.plot(np.sqrt(history.history[\"val_loss\"]))\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epocas\")\n",
    "plt.legend([\"Treino\", \"Validação\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "pred = np.array(pred).flatten()\n",
    "erro = pred - np.array(y_test[:, 0]).flatten()\n",
    "erro = np.abs(erro)\n",
    "\n",
    "acerto = 0\n",
    "for i in erro:\n",
    "    if i < 0.5:\n",
    "        acerto += 1\n",
    "\n",
    "best_pred = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(reais, preditos, labels):\n",
    "    \"\"\"\n",
    "    Uma função que retorna a matriz de confusão para uma classificação binária\n",
    "    \n",
    "    Args:\n",
    "        reais (list): lista de valores reais\n",
    "        preditos (list): lista de valores preditos pelo modelos\n",
    "        labels (list): lista de labels a serem avaliados.\n",
    "            É importante que ela esteja presente, pois usaremos ela para entender\n",
    "            quem é a classe positiva e quem é a classe negativa\n",
    "    \n",
    "    Returns:\n",
    "        Um numpy.array, no formato:\n",
    "            numpy.array([\n",
    "                [ tp, fp ],\n",
    "                [ fn, tn ]\n",
    "            ])\n",
    "    \"\"\"\n",
    "    # não implementado\n",
    "    if len(labels) > 2:\n",
    "        return None\n",
    "\n",
    "    if len(reais) != len(preditos):\n",
    "        return None\n",
    "    \n",
    "    # considerando a primeira classe como a positiva, e a segunda a negativa\n",
    "    true_class = labels[0]\n",
    "    negative_class = labels[1]\n",
    "\n",
    "    # valores preditos corretamente\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    \n",
    "    # valores preditos incorretamente\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for (indice, v_real) in enumerate(reais):\n",
    "        v_predito = preditos[indice]\n",
    "\n",
    "        # se trata de um valor real da classe positiva\n",
    "        if v_real == true_class:\n",
    "            tp += 1 if v_predito == v_real else 0\n",
    "            fp += 1 if v_predito != v_real else 0\n",
    "        else:\n",
    "            tn += 1 if v_predito == v_real else 0\n",
    "            fn += 1 if v_predito != v_real else 0\n",
    "    \n",
    "    return np.array([\n",
    "        # valores da classe positiva\n",
    "        [ tp, fp ],\n",
    "        # valores da classe negativa\n",
    "        [ fn, tn ]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 0s 746us/step - loss: 0.3189 - acc: 0.4846 - val_loss: 0.2903 - val_acc: 0.4683\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.2739 - acc: 0.4897 - val_loss: 0.2597 - val_acc: 0.4841\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.2528 - acc: 0.5205 - val_loss: 0.2453 - val_acc: 0.5079\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2446 - acc: 0.5719 - val_loss: 0.2379 - val_acc: 0.5873\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.2402 - acc: 0.6336 - val_loss: 0.2336 - val_acc: 0.6508\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2370 - acc: 0.6592 - val_loss: 0.2297 - val_acc: 0.6587\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.2340 - acc: 0.6764 - val_loss: 0.2261 - val_acc: 0.6905\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.2310 - acc: 0.6969 - val_loss: 0.2229 - val_acc: 0.6905\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.2283 - acc: 0.6986 - val_loss: 0.2193 - val_acc: 0.7302\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2255 - acc: 0.7021 - val_loss: 0.2163 - val_acc: 0.7381\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.2231 - acc: 0.7038 - val_loss: 0.2131 - val_acc: 0.7460\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.2205 - acc: 0.7106 - val_loss: 0.2103 - val_acc: 0.7460\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.2180 - acc: 0.7055 - val_loss: 0.2077 - val_acc: 0.7540\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.2159 - acc: 0.7140 - val_loss: 0.2051 - val_acc: 0.7540\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.2137 - acc: 0.7106 - val_loss: 0.2025 - val_acc: 0.7540\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.2117 - acc: 0.7055 - val_loss: 0.2002 - val_acc: 0.7540\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.2098 - acc: 0.7072 - val_loss: 0.1980 - val_acc: 0.7540\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.2078 - acc: 0.7140 - val_loss: 0.1959 - val_acc: 0.7619\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 129us/step - loss: 0.2060 - acc: 0.7209 - val_loss: 0.1938 - val_acc: 0.7619\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.2044 - acc: 0.7277 - val_loss: 0.1919 - val_acc: 0.7619\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.2027 - acc: 0.7363 - val_loss: 0.1900 - val_acc: 0.7619\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.2012 - acc: 0.7277 - val_loss: 0.1880 - val_acc: 0.7619\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1996 - acc: 0.7277 - val_loss: 0.1863 - val_acc: 0.7619\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1980 - acc: 0.7312 - val_loss: 0.1846 - val_acc: 0.7619\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1967 - acc: 0.7346 - val_loss: 0.1830 - val_acc: 0.7619\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1954 - acc: 0.7586 - val_loss: 0.1816 - val_acc: 0.7857\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1940 - acc: 0.7586 - val_loss: 0.1800 - val_acc: 0.7857\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1928 - acc: 0.7603 - val_loss: 0.1786 - val_acc: 0.7857\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1916 - acc: 0.7551 - val_loss: 0.1771 - val_acc: 0.7857\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1904 - acc: 0.7586 - val_loss: 0.1759 - val_acc: 0.7857\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1893 - acc: 0.7620 - val_loss: 0.1747 - val_acc: 0.7857\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1883 - acc: 0.7637 - val_loss: 0.1735 - val_acc: 0.7857\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1873 - acc: 0.7654 - val_loss: 0.1724 - val_acc: 0.7857\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1862 - acc: 0.7671 - val_loss: 0.1712 - val_acc: 0.7857\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1853 - acc: 0.7671 - val_loss: 0.1701 - val_acc: 0.7857\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1843 - acc: 0.7671 - val_loss: 0.1691 - val_acc: 0.7857\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1835 - acc: 0.7688 - val_loss: 0.1681 - val_acc: 0.7857\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1826 - acc: 0.7688 - val_loss: 0.1671 - val_acc: 0.7857\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1818 - acc: 0.7688 - val_loss: 0.1662 - val_acc: 0.7857\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1810 - acc: 0.7705 - val_loss: 0.1653 - val_acc: 0.7857\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1804 - acc: 0.7705 - val_loss: 0.1644 - val_acc: 0.7857\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1795 - acc: 0.7705 - val_loss: 0.1636 - val_acc: 0.7857\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1788 - acc: 0.7705 - val_loss: 0.1628 - val_acc: 0.7937\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1781 - acc: 0.7723 - val_loss: 0.1620 - val_acc: 0.7857\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1775 - acc: 0.7705 - val_loss: 0.1613 - val_acc: 0.8016\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1768 - acc: 0.7723 - val_loss: 0.1606 - val_acc: 0.7937\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1762 - acc: 0.7705 - val_loss: 0.1599 - val_acc: 0.7937\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1756 - acc: 0.7723 - val_loss: 0.1592 - val_acc: 0.7937\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1749 - acc: 0.7740 - val_loss: 0.1585 - val_acc: 0.7937\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1746 - acc: 0.7723 - val_loss: 0.1578 - val_acc: 0.7937\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1739 - acc: 0.7740 - val_loss: 0.1573 - val_acc: 0.8016\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1733 - acc: 0.7740 - val_loss: 0.1566 - val_acc: 0.7937\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1727 - acc: 0.7723 - val_loss: 0.1561 - val_acc: 0.7937\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1722 - acc: 0.7740 - val_loss: 0.1555 - val_acc: 0.8016\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1718 - acc: 0.7740 - val_loss: 0.1550 - val_acc: 0.7937\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1713 - acc: 0.7740 - val_loss: 0.1545 - val_acc: 0.8095\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1709 - acc: 0.7842 - val_loss: 0.1540 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1704 - acc: 0.7842 - val_loss: 0.1535 - val_acc: 0.8095\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1700 - acc: 0.7842 - val_loss: 0.1530 - val_acc: 0.8095\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1696 - acc: 0.7842 - val_loss: 0.1526 - val_acc: 0.8095\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 85us/step - loss: 0.1693 - acc: 0.7774 - val_loss: 0.1521 - val_acc: 0.7937\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1688 - acc: 0.7791 - val_loss: 0.1517 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1685 - acc: 0.7842 - val_loss: 0.1513 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1680 - acc: 0.7842 - val_loss: 0.1509 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1677 - acc: 0.7842 - val_loss: 0.1505 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1673 - acc: 0.7842 - val_loss: 0.1501 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1670 - acc: 0.7842 - val_loss: 0.1498 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1666 - acc: 0.7842 - val_loss: 0.1494 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1663 - acc: 0.7842 - val_loss: 0.1491 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1663 - acc: 0.7825 - val_loss: 0.1487 - val_acc: 0.8095\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1657 - acc: 0.7842 - val_loss: 0.1484 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1654 - acc: 0.7842 - val_loss: 0.1480 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1651 - acc: 0.7842 - val_loss: 0.1477 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1650 - acc: 0.7808 - val_loss: 0.1474 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1645 - acc: 0.7842 - val_loss: 0.1472 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1642 - acc: 0.7842 - val_loss: 0.1469 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1641 - acc: 0.7808 - val_loss: 0.1467 - val_acc: 0.8095\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1637 - acc: 0.7842 - val_loss: 0.1463 - val_acc: 0.8095\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1635 - acc: 0.7842 - val_loss: 0.1461 - val_acc: 0.8095\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1634 - acc: 0.7842 - val_loss: 0.1459 - val_acc: 0.8095\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1631 - acc: 0.7825 - val_loss: 0.1456 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1628 - acc: 0.7842 - val_loss: 0.1453 - val_acc: 0.8095\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1626 - acc: 0.7825 - val_loss: 0.1451 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1623 - acc: 0.7842 - val_loss: 0.1449 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1621 - acc: 0.7842 - val_loss: 0.1447 - val_acc: 0.8095\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1619 - acc: 0.7842 - val_loss: 0.1445 - val_acc: 0.8095\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1616 - acc: 0.7842 - val_loss: 0.1443 - val_acc: 0.8095\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1615 - acc: 0.7842 - val_loss: 0.1440 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1613 - acc: 0.7842 - val_loss: 0.1438 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1611 - acc: 0.7842 - val_loss: 0.1437 - val_acc: 0.8095\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1608 - acc: 0.7842 - val_loss: 0.1434 - val_acc: 0.8095\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1608 - acc: 0.7825 - val_loss: 0.1432 - val_acc: 0.8095\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1605 - acc: 0.7842 - val_loss: 0.1431 - val_acc: 0.8095\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 129us/step - loss: 0.1603 - acc: 0.7842 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1602 - acc: 0.7842 - val_loss: 0.1427 - val_acc: 0.8095\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1600 - acc: 0.7825 - val_loss: 0.1426 - val_acc: 0.8095\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1598 - acc: 0.7808 - val_loss: 0.1425 - val_acc: 0.8095\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1596 - acc: 0.7842 - val_loss: 0.1422 - val_acc: 0.8095\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1595 - acc: 0.7842 - val_loss: 0.1421 - val_acc: 0.8095\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1593 - acc: 0.7842 - val_loss: 0.1419 - val_acc: 0.8175\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1593 - acc: 0.7842 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1589 - acc: 0.7825 - val_loss: 0.1416 - val_acc: 0.8175\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1588 - acc: 0.7825 - val_loss: 0.1415 - val_acc: 0.8175\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1587 - acc: 0.7842 - val_loss: 0.1413 - val_acc: 0.8175\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1585 - acc: 0.7825 - val_loss: 0.1412 - val_acc: 0.8175\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1585 - acc: 0.7808 - val_loss: 0.1411 - val_acc: 0.8175\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1584 - acc: 0.7842 - val_loss: 0.1410 - val_acc: 0.8175\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1581 - acc: 0.7842 - val_loss: 0.1408 - val_acc: 0.8175\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1579 - acc: 0.7860 - val_loss: 0.1407 - val_acc: 0.8175\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1578 - acc: 0.7842 - val_loss: 0.1405 - val_acc: 0.8175\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1576 - acc: 0.7860 - val_loss: 0.1405 - val_acc: 0.8175\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1578 - acc: 0.7860 - val_loss: 0.1403 - val_acc: 0.8175\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1574 - acc: 0.7808 - val_loss: 0.1403 - val_acc: 0.8175\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1573 - acc: 0.7808 - val_loss: 0.1401 - val_acc: 0.8175\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1573 - acc: 0.7825 - val_loss: 0.1400 - val_acc: 0.8175\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1569 - acc: 0.7825 - val_loss: 0.1399 - val_acc: 0.8175\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1568 - acc: 0.7860 - val_loss: 0.1398 - val_acc: 0.8175\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1567 - acc: 0.7825 - val_loss: 0.1397 - val_acc: 0.8175\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1567 - acc: 0.7825 - val_loss: 0.1395 - val_acc: 0.8175\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1564 - acc: 0.7842 - val_loss: 0.1395 - val_acc: 0.8175\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 97us/step - loss: 0.1563 - acc: 0.7825 - val_loss: 0.1394 - val_acc: 0.8175\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1562 - acc: 0.7842 - val_loss: 0.1392 - val_acc: 0.8175\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1561 - acc: 0.7842 - val_loss: 0.1391 - val_acc: 0.8175\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1560 - acc: 0.7825 - val_loss: 0.1390 - val_acc: 0.8175\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1562 - acc: 0.7842 - val_loss: 0.1389 - val_acc: 0.8175\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1558 - acc: 0.7791 - val_loss: 0.1389 - val_acc: 0.8175\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1556 - acc: 0.7808 - val_loss: 0.1388 - val_acc: 0.8175\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1557 - acc: 0.7842 - val_loss: 0.1386 - val_acc: 0.8175\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1555 - acc: 0.7825 - val_loss: 0.1386 - val_acc: 0.8175\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1553 - acc: 0.7808 - val_loss: 0.1385 - val_acc: 0.8175\n",
      "Epoch 131/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1552 - acc: 0.7808 - val_loss: 0.1384 - val_acc: 0.8175\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1551 - acc: 0.7825 - val_loss: 0.1384 - val_acc: 0.8175\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1550 - acc: 0.7808 - val_loss: 0.1383 - val_acc: 0.8175\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1548 - acc: 0.7825 - val_loss: 0.1381 - val_acc: 0.8175\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1548 - acc: 0.7825 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1546 - acc: 0.7825 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1546 - acc: 0.7808 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1545 - acc: 0.7842 - val_loss: 0.1378 - val_acc: 0.8175\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1545 - acc: 0.7842 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 140/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1543 - acc: 0.7842 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 141/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1542 - acc: 0.7842 - val_loss: 0.1376 - val_acc: 0.8175\n",
      "Epoch 142/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1540 - acc: 0.7825 - val_loss: 0.1376 - val_acc: 0.8175\n",
      "Epoch 143/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1540 - acc: 0.7842 - val_loss: 0.1375 - val_acc: 0.8175\n",
      "Epoch 144/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1539 - acc: 0.7825 - val_loss: 0.1374 - val_acc: 0.8175\n",
      "Epoch 145/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1538 - acc: 0.7842 - val_loss: 0.1374 - val_acc: 0.8175\n",
      "Epoch 146/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1536 - acc: 0.7860 - val_loss: 0.1372 - val_acc: 0.8175\n",
      "Epoch 147/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1535 - acc: 0.7860 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 148/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1535 - acc: 0.7860 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 149/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1534 - acc: 0.7860 - val_loss: 0.1370 - val_acc: 0.8175\n",
      "Epoch 150/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1533 - acc: 0.7860 - val_loss: 0.1370 - val_acc: 0.8175\n",
      "Epoch 151/200\n",
      "584/584 [==============================] - 0s 131us/step - loss: 0.1533 - acc: 0.7877 - val_loss: 0.1370 - val_acc: 0.8175\n",
      "Epoch 152/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1531 - acc: 0.7860 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 153/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1530 - acc: 0.7860 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 154/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1529 - acc: 0.7860 - val_loss: 0.1367 - val_acc: 0.8175\n",
      "Epoch 155/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1528 - acc: 0.7860 - val_loss: 0.1366 - val_acc: 0.8175\n",
      "Epoch 156/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1528 - acc: 0.7860 - val_loss: 0.1366 - val_acc: 0.8254\n",
      "Epoch 157/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1527 - acc: 0.7860 - val_loss: 0.1365 - val_acc: 0.8175\n",
      "Epoch 158/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1526 - acc: 0.7842 - val_loss: 0.1365 - val_acc: 0.8175\n",
      "Epoch 159/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1525 - acc: 0.7860 - val_loss: 0.1364 - val_acc: 0.8254\n",
      "Epoch 160/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1524 - acc: 0.7860 - val_loss: 0.1363 - val_acc: 0.8254\n",
      "Epoch 161/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1523 - acc: 0.7842 - val_loss: 0.1363 - val_acc: 0.8175\n",
      "Epoch 162/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1523 - acc: 0.7842 - val_loss: 0.1363 - val_acc: 0.8175\n",
      "Epoch 163/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1521 - acc: 0.7842 - val_loss: 0.1362 - val_acc: 0.8175\n",
      "Epoch 164/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1521 - acc: 0.7842 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 165/200\n",
      "584/584 [==============================] - 0s 56us/step - loss: 0.1521 - acc: 0.7860 - val_loss: 0.1360 - val_acc: 0.8254\n",
      "Epoch 166/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1522 - acc: 0.7877 - val_loss: 0.1360 - val_acc: 0.8254\n",
      "Epoch 167/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1520 - acc: 0.7860 - val_loss: 0.1362 - val_acc: 0.8175\n",
      "Epoch 168/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1518 - acc: 0.7860 - val_loss: 0.1360 - val_acc: 0.8175\n",
      "Epoch 169/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1517 - acc: 0.7860 - val_loss: 0.1359 - val_acc: 0.8254\n",
      "Epoch 170/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1516 - acc: 0.7860 - val_loss: 0.1358 - val_acc: 0.8254\n",
      "Epoch 171/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1516 - acc: 0.7860 - val_loss: 0.1358 - val_acc: 0.8254\n",
      "Epoch 172/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1514 - acc: 0.7860 - val_loss: 0.1357 - val_acc: 0.8254\n",
      "Epoch 173/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1515 - acc: 0.7877 - val_loss: 0.1356 - val_acc: 0.8254\n",
      "Epoch 174/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1515 - acc: 0.7842 - val_loss: 0.1357 - val_acc: 0.8175\n",
      "Epoch 175/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1512 - acc: 0.7860 - val_loss: 0.1356 - val_acc: 0.8254\n",
      "Epoch 176/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1511 - acc: 0.7877 - val_loss: 0.1355 - val_acc: 0.8254\n",
      "Epoch 177/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1511 - acc: 0.7877 - val_loss: 0.1354 - val_acc: 0.8254\n",
      "Epoch 178/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1510 - acc: 0.7877 - val_loss: 0.1354 - val_acc: 0.8254\n",
      "Epoch 179/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1510 - acc: 0.7877 - val_loss: 0.1354 - val_acc: 0.8254\n",
      "Epoch 180/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1508 - acc: 0.7860 - val_loss: 0.1353 - val_acc: 0.8254\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 66us/step - loss: 0.1508 - acc: 0.7860 - val_loss: 0.1354 - val_acc: 0.8254\n",
      "Epoch 182/200\n",
      "584/584 [==============================] - 0s 56us/step - loss: 0.1508 - acc: 0.7877 - val_loss: 0.1352 - val_acc: 0.8254\n",
      "Epoch 183/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1507 - acc: 0.7860 - val_loss: 0.1352 - val_acc: 0.8254\n",
      "Epoch 184/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1506 - acc: 0.7877 - val_loss: 0.1351 - val_acc: 0.8254\n",
      "Epoch 185/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1505 - acc: 0.7877 - val_loss: 0.1351 - val_acc: 0.8254\n",
      "Epoch 186/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1504 - acc: 0.7860 - val_loss: 0.1351 - val_acc: 0.8254\n",
      "Epoch 00186: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 0s 837us/step - loss: 0.2433 - acc: 0.4949 - val_loss: 0.2240 - val_acc: 0.5635\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2229 - acc: 0.6729 - val_loss: 0.2126 - val_acc: 0.8175\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.2178 - acc: 0.7654 - val_loss: 0.2093 - val_acc: 0.8095\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.2156 - acc: 0.7705 - val_loss: 0.2075 - val_acc: 0.8095\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.2138 - acc: 0.7723 - val_loss: 0.2058 - val_acc: 0.8095\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.2125 - acc: 0.7723 - val_loss: 0.2041 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.2109 - acc: 0.7791 - val_loss: 0.2023 - val_acc: 0.8095\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.2094 - acc: 0.7757 - val_loss: 0.2005 - val_acc: 0.8095\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.2078 - acc: 0.7740 - val_loss: 0.1988 - val_acc: 0.8095\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.2064 - acc: 0.7723 - val_loss: 0.1971 - val_acc: 0.8095\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.2049 - acc: 0.7723 - val_loss: 0.1954 - val_acc: 0.8095\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.2037 - acc: 0.7723 - val_loss: 0.1937 - val_acc: 0.8095\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.2020 - acc: 0.7705 - val_loss: 0.1922 - val_acc: 0.8095\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.2008 - acc: 0.7757 - val_loss: 0.1905 - val_acc: 0.8095\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1993 - acc: 0.7757 - val_loss: 0.1889 - val_acc: 0.8095\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1980 - acc: 0.7740 - val_loss: 0.1873 - val_acc: 0.8095\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1967 - acc: 0.7723 - val_loss: 0.1858 - val_acc: 0.8095\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1953 - acc: 0.7740 - val_loss: 0.1844 - val_acc: 0.8095\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1941 - acc: 0.7791 - val_loss: 0.1829 - val_acc: 0.8095\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1929 - acc: 0.7740 - val_loss: 0.1815 - val_acc: 0.8095\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1916 - acc: 0.7740 - val_loss: 0.1801 - val_acc: 0.8095\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1904 - acc: 0.7757 - val_loss: 0.1787 - val_acc: 0.8095\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1892 - acc: 0.7757 - val_loss: 0.1774 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1881 - acc: 0.7757 - val_loss: 0.1760 - val_acc: 0.8095\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1871 - acc: 0.7774 - val_loss: 0.1749 - val_acc: 0.8016\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1858 - acc: 0.7791 - val_loss: 0.1736 - val_acc: 0.8095\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1848 - acc: 0.7774 - val_loss: 0.1724 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1839 - acc: 0.7757 - val_loss: 0.1712 - val_acc: 0.8095\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1828 - acc: 0.7791 - val_loss: 0.1701 - val_acc: 0.8095\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1820 - acc: 0.7774 - val_loss: 0.1690 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1810 - acc: 0.7740 - val_loss: 0.1679 - val_acc: 0.8175\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1800 - acc: 0.7757 - val_loss: 0.1670 - val_acc: 0.8095\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1792 - acc: 0.7808 - val_loss: 0.1661 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1784 - acc: 0.7774 - val_loss: 0.1650 - val_acc: 0.8175\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1774 - acc: 0.7791 - val_loss: 0.1642 - val_acc: 0.8095\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1766 - acc: 0.7791 - val_loss: 0.1633 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1759 - acc: 0.7808 - val_loss: 0.1624 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1750 - acc: 0.7791 - val_loss: 0.1615 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1744 - acc: 0.7757 - val_loss: 0.1607 - val_acc: 0.8175\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1737 - acc: 0.7757 - val_loss: 0.1600 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1730 - acc: 0.7791 - val_loss: 0.1592 - val_acc: 0.8175\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1723 - acc: 0.7825 - val_loss: 0.1585 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1721 - acc: 0.7757 - val_loss: 0.1577 - val_acc: 0.8175\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1711 - acc: 0.7825 - val_loss: 0.1574 - val_acc: 0.8016\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1705 - acc: 0.7808 - val_loss: 0.1566 - val_acc: 0.8175\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1700 - acc: 0.7791 - val_loss: 0.1559 - val_acc: 0.8175\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1694 - acc: 0.7774 - val_loss: 0.1554 - val_acc: 0.8175\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1689 - acc: 0.7808 - val_loss: 0.1548 - val_acc: 0.8175\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1684 - acc: 0.7791 - val_loss: 0.1542 - val_acc: 0.8175\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1679 - acc: 0.7791 - val_loss: 0.1537 - val_acc: 0.8175\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1675 - acc: 0.7825 - val_loss: 0.1534 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1670 - acc: 0.7825 - val_loss: 0.1530 - val_acc: 0.8016\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1666 - acc: 0.7825 - val_loss: 0.1523 - val_acc: 0.8175\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1661 - acc: 0.7757 - val_loss: 0.1519 - val_acc: 0.8175\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 58us/step - loss: 0.1657 - acc: 0.7808 - val_loss: 0.1515 - val_acc: 0.8175\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1653 - acc: 0.7808 - val_loss: 0.1511 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1650 - acc: 0.7825 - val_loss: 0.1509 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1646 - acc: 0.7825 - val_loss: 0.1503 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1642 - acc: 0.7791 - val_loss: 0.1499 - val_acc: 0.8175\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1639 - acc: 0.7842 - val_loss: 0.1497 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1637 - acc: 0.7825 - val_loss: 0.1495 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1633 - acc: 0.7774 - val_loss: 0.1489 - val_acc: 0.8175\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1630 - acc: 0.7774 - val_loss: 0.1486 - val_acc: 0.8175\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1627 - acc: 0.7808 - val_loss: 0.1484 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1624 - acc: 0.7842 - val_loss: 0.1480 - val_acc: 0.8175\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1623 - acc: 0.7825 - val_loss: 0.1481 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1620 - acc: 0.7825 - val_loss: 0.1477 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1616 - acc: 0.7825 - val_loss: 0.1474 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1616 - acc: 0.7825 - val_loss: 0.1473 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1612 - acc: 0.7825 - val_loss: 0.1468 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1609 - acc: 0.7808 - val_loss: 0.1467 - val_acc: 0.8175\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1606 - acc: 0.7842 - val_loss: 0.1465 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1605 - acc: 0.7842 - val_loss: 0.1464 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1602 - acc: 0.7825 - val_loss: 0.1461 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1601 - acc: 0.7825 - val_loss: 0.1458 - val_acc: 0.8175\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1598 - acc: 0.7825 - val_loss: 0.1457 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1596 - acc: 0.7791 - val_loss: 0.1453 - val_acc: 0.8175\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1596 - acc: 0.7808 - val_loss: 0.1453 - val_acc: 0.8175\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1593 - acc: 0.7791 - val_loss: 0.1451 - val_acc: 0.8175\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1590 - acc: 0.7791 - val_loss: 0.1449 - val_acc: 0.8175\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1590 - acc: 0.7791 - val_loss: 0.1449 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1587 - acc: 0.7791 - val_loss: 0.1446 - val_acc: 0.8175\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1586 - acc: 0.7791 - val_loss: 0.1445 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1584 - acc: 0.7791 - val_loss: 0.1443 - val_acc: 0.8175\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1583 - acc: 0.7825 - val_loss: 0.1443 - val_acc: 0.8095\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1583 - acc: 0.7791 - val_loss: 0.1441 - val_acc: 0.8175\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1579 - acc: 0.7791 - val_loss: 0.1441 - val_acc: 0.8095\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1579 - acc: 0.7808 - val_loss: 0.1439 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1577 - acc: 0.7791 - val_loss: 0.1438 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1576 - acc: 0.7842 - val_loss: 0.1439 - val_acc: 0.8095\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1574 - acc: 0.7842 - val_loss: 0.1436 - val_acc: 0.8095\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1574 - acc: 0.7808 - val_loss: 0.1436 - val_acc: 0.8095\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1572 - acc: 0.7825 - val_loss: 0.1434 - val_acc: 0.8095\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1571 - acc: 0.7825 - val_loss: 0.1433 - val_acc: 0.8095\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1569 - acc: 0.7791 - val_loss: 0.1432 - val_acc: 0.8095\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1569 - acc: 0.7791 - val_loss: 0.1429 - val_acc: 0.8175\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1568 - acc: 0.7791 - val_loss: 0.1430 - val_acc: 0.8095\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1567 - acc: 0.7791 - val_loss: 0.1428 - val_acc: 0.8175\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1565 - acc: 0.7808 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1565 - acc: 0.7808 - val_loss: 0.1426 - val_acc: 0.8175\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1563 - acc: 0.7791 - val_loss: 0.1427 - val_acc: 0.8095\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1562 - acc: 0.7808 - val_loss: 0.1426 - val_acc: 0.8095\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1561 - acc: 0.7808 - val_loss: 0.1426 - val_acc: 0.8095\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1560 - acc: 0.7808 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1559 - acc: 0.7791 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1558 - acc: 0.7791 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1558 - acc: 0.7808 - val_loss: 0.1422 - val_acc: 0.8095\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1556 - acc: 0.7825 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1555 - acc: 0.7808 - val_loss: 0.1421 - val_acc: 0.8095\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1554 - acc: 0.7791 - val_loss: 0.1420 - val_acc: 0.8095\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1554 - acc: 0.7808 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1554 - acc: 0.7808 - val_loss: 0.1418 - val_acc: 0.8095\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1552 - acc: 0.7791 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1551 - acc: 0.7808 - val_loss: 0.1418 - val_acc: 0.8095\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 70us/step - loss: 0.1551 - acc: 0.7825 - val_loss: 0.1418 - val_acc: 0.8095\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1551 - acc: 0.7825 - val_loss: 0.1418 - val_acc: 0.8095\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1549 - acc: 0.7791 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1548 - acc: 0.7808 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1547 - acc: 0.7791 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1547 - acc: 0.7825 - val_loss: 0.1414 - val_acc: 0.8095\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1545 - acc: 0.7825 - val_loss: 0.1414 - val_acc: 0.8095\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1545 - acc: 0.7825 - val_loss: 0.1414 - val_acc: 0.8095\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 55us/step - loss: 0.1544 - acc: 0.7808 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1544 - acc: 0.7825 - val_loss: 0.1415 - val_acc: 0.8095\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1543 - acc: 0.7825 - val_loss: 0.1412 - val_acc: 0.8095\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1542 - acc: 0.7825 - val_loss: 0.1411 - val_acc: 0.8095\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1543 - acc: 0.7842 - val_loss: 0.1409 - val_acc: 0.8175\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1541 - acc: 0.7842 - val_loss: 0.1411 - val_acc: 0.8095\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1540 - acc: 0.7842 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1539 - acc: 0.7825 - val_loss: 0.1411 - val_acc: 0.8095\n",
      "Epoch 131/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1540 - acc: 0.7842 - val_loss: 0.1408 - val_acc: 0.8095\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 56us/step - loss: 0.1538 - acc: 0.7825 - val_loss: 0.1410 - val_acc: 0.8095\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1537 - acc: 0.7842 - val_loss: 0.1410 - val_acc: 0.8095\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1537 - acc: 0.7842 - val_loss: 0.1411 - val_acc: 0.8095\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1536 - acc: 0.7842 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1536 - acc: 0.7825 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1536 - acc: 0.7860 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1534 - acc: 0.7825 - val_loss: 0.1408 - val_acc: 0.8095\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1535 - acc: 0.7825 - val_loss: 0.1405 - val_acc: 0.8095\n",
      "Epoch 140/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1535 - acc: 0.7860 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 141/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1532 - acc: 0.7842 - val_loss: 0.1407 - val_acc: 0.8095\n",
      "Epoch 142/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1533 - acc: 0.7860 - val_loss: 0.1402 - val_acc: 0.8175\n",
      "Epoch 143/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1531 - acc: 0.7860 - val_loss: 0.1404 - val_acc: 0.8095\n",
      "Epoch 144/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1530 - acc: 0.7842 - val_loss: 0.1405 - val_acc: 0.8095\n",
      "Epoch 145/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1530 - acc: 0.7842 - val_loss: 0.1405 - val_acc: 0.8095\n",
      "Epoch 146/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1530 - acc: 0.7842 - val_loss: 0.1404 - val_acc: 0.8095\n",
      "Epoch 00146: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 0s 852us/step - loss: 2.1295 - acc: 0.0000e+00 - val_loss: 1.8565 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 1.7051 - acc: 0.0000e+00 - val_loss: 1.4825 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 1.3649 - acc: 0.0240 - val_loss: 1.1810 - val_acc: 0.1111\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 1.0919 - acc: 0.2877 - val_loss: 0.9457 - val_acc: 0.5317\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.8790 - acc: 0.5154 - val_loss: 0.7629 - val_acc: 0.5317\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.7127 - acc: 0.5154 - val_loss: 0.6182 - val_acc: 0.5317\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.5819 - acc: 0.5154 - val_loss: 0.5080 - val_acc: 0.5317\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.4827 - acc: 0.5154 - val_loss: 0.4231 - val_acc: 0.5317\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.4074 - acc: 0.5154 - val_loss: 0.3592 - val_acc: 0.5317\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.3494 - acc: 0.5154 - val_loss: 0.3130 - val_acc: 0.5317\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.3073 - acc: 0.5154 - val_loss: 0.2781 - val_acc: 0.5317\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2765 - acc: 0.5154 - val_loss: 0.2526 - val_acc: 0.5317\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.2534 - acc: 0.5154 - val_loss: 0.2350 - val_acc: 0.5317\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2372 - acc: 0.5154 - val_loss: 0.2222 - val_acc: 0.5317\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.2261 - acc: 0.5154 - val_loss: 0.2132 - val_acc: 0.5317\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.2180 - acc: 0.5205 - val_loss: 0.2074 - val_acc: 0.5397\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.2128 - acc: 0.5531 - val_loss: 0.2031 - val_acc: 0.5952\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.2087 - acc: 0.6233 - val_loss: 0.2005 - val_acc: 0.6905\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.2063 - acc: 0.6935 - val_loss: 0.1984 - val_acc: 0.7302\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2043 - acc: 0.7534 - val_loss: 0.1972 - val_acc: 0.7778\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2032 - acc: 0.7808 - val_loss: 0.1961 - val_acc: 0.8175\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2022 - acc: 0.7928 - val_loss: 0.1954 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2015 - acc: 0.7962 - val_loss: 0.1948 - val_acc: 0.8175\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2008 - acc: 0.7997 - val_loss: 0.1943 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2003 - acc: 0.8048 - val_loss: 0.1938 - val_acc: 0.8175\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1998 - acc: 0.8116 - val_loss: 0.1933 - val_acc: 0.8254\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1993 - acc: 0.8065 - val_loss: 0.1928 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1989 - acc: 0.8065 - val_loss: 0.1923 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1985 - acc: 0.8116 - val_loss: 0.1919 - val_acc: 0.8095\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1981 - acc: 0.8048 - val_loss: 0.1915 - val_acc: 0.8175\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1976 - acc: 0.8065 - val_loss: 0.1909 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1972 - acc: 0.8031 - val_loss: 0.1905 - val_acc: 0.8175\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1969 - acc: 0.8082 - val_loss: 0.1900 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1964 - acc: 0.8099 - val_loss: 0.1895 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1960 - acc: 0.8048 - val_loss: 0.1891 - val_acc: 0.8175\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1955 - acc: 0.8065 - val_loss: 0.1886 - val_acc: 0.8175\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1951 - acc: 0.8065 - val_loss: 0.1881 - val_acc: 0.8175\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1947 - acc: 0.8065 - val_loss: 0.1876 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1943 - acc: 0.8116 - val_loss: 0.1871 - val_acc: 0.8095\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1939 - acc: 0.8116 - val_loss: 0.1866 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1934 - acc: 0.8099 - val_loss: 0.1861 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1930 - acc: 0.8065 - val_loss: 0.1857 - val_acc: 0.8175\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1926 - acc: 0.8065 - val_loss: 0.1852 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1922 - acc: 0.8065 - val_loss: 0.1847 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1918 - acc: 0.8065 - val_loss: 0.1842 - val_acc: 0.8095\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1914 - acc: 0.8065 - val_loss: 0.1837 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1909 - acc: 0.8082 - val_loss: 0.1833 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1905 - acc: 0.8048 - val_loss: 0.1828 - val_acc: 0.8175\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1902 - acc: 0.8099 - val_loss: 0.1823 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1897 - acc: 0.8048 - val_loss: 0.1818 - val_acc: 0.8095\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1892 - acc: 0.8065 - val_loss: 0.1813 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1888 - acc: 0.8014 - val_loss: 0.1808 - val_acc: 0.8175\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1884 - acc: 0.8031 - val_loss: 0.1803 - val_acc: 0.8095\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1880 - acc: 0.8065 - val_loss: 0.1798 - val_acc: 0.8095\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1875 - acc: 0.8065 - val_loss: 0.1794 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1872 - acc: 0.7979 - val_loss: 0.1789 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1867 - acc: 0.7997 - val_loss: 0.1784 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1863 - acc: 0.8014 - val_loss: 0.1779 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1859 - acc: 0.7997 - val_loss: 0.1774 - val_acc: 0.8095\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1855 - acc: 0.8014 - val_loss: 0.1770 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1850 - acc: 0.8048 - val_loss: 0.1764 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1847 - acc: 0.8014 - val_loss: 0.1760 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1842 - acc: 0.8065 - val_loss: 0.1755 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1837 - acc: 0.8065 - val_loss: 0.1750 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1833 - acc: 0.8065 - val_loss: 0.1746 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1830 - acc: 0.7979 - val_loss: 0.1741 - val_acc: 0.8175\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1825 - acc: 0.7979 - val_loss: 0.1736 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1821 - acc: 0.8048 - val_loss: 0.1731 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1817 - acc: 0.8031 - val_loss: 0.1726 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1815 - acc: 0.7945 - val_loss: 0.1722 - val_acc: 0.8254\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1809 - acc: 0.7997 - val_loss: 0.1717 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1805 - acc: 0.7997 - val_loss: 0.1712 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1801 - acc: 0.7962 - val_loss: 0.1708 - val_acc: 0.8175\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1796 - acc: 0.8014 - val_loss: 0.1703 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1793 - acc: 0.8031 - val_loss: 0.1698 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1789 - acc: 0.7962 - val_loss: 0.1694 - val_acc: 0.8175\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1784 - acc: 0.7997 - val_loss: 0.1689 - val_acc: 0.8095\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1781 - acc: 0.8048 - val_loss: 0.1685 - val_acc: 0.8095\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1777 - acc: 0.7945 - val_loss: 0.1680 - val_acc: 0.8254\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1773 - acc: 0.7979 - val_loss: 0.1676 - val_acc: 0.8254\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1769 - acc: 0.7997 - val_loss: 0.1671 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1764 - acc: 0.7962 - val_loss: 0.1666 - val_acc: 0.8175\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1760 - acc: 0.7979 - val_loss: 0.1662 - val_acc: 0.8175\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1756 - acc: 0.7979 - val_loss: 0.1657 - val_acc: 0.8175\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1752 - acc: 0.7979 - val_loss: 0.1653 - val_acc: 0.8175\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1749 - acc: 0.7962 - val_loss: 0.1648 - val_acc: 0.8175\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1746 - acc: 0.7962 - val_loss: 0.1644 - val_acc: 0.8175\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1741 - acc: 0.8014 - val_loss: 0.1640 - val_acc: 0.8175\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1737 - acc: 0.8014 - val_loss: 0.1635 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1734 - acc: 0.7997 - val_loss: 0.1631 - val_acc: 0.8175\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1731 - acc: 0.7962 - val_loss: 0.1627 - val_acc: 0.8254\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1725 - acc: 0.7945 - val_loss: 0.1622 - val_acc: 0.8254\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1722 - acc: 0.7997 - val_loss: 0.1618 - val_acc: 0.8175\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1718 - acc: 0.8014 - val_loss: 0.1613 - val_acc: 0.8175\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1715 - acc: 0.8031 - val_loss: 0.1610 - val_acc: 0.8095\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1710 - acc: 0.8031 - val_loss: 0.1605 - val_acc: 0.8175\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1707 - acc: 0.7962 - val_loss: 0.1601 - val_acc: 0.8175\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1703 - acc: 0.7979 - val_loss: 0.1597 - val_acc: 0.8175\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1699 - acc: 0.7997 - val_loss: 0.1593 - val_acc: 0.8175\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1696 - acc: 0.8014 - val_loss: 0.1589 - val_acc: 0.8175\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1692 - acc: 0.7979 - val_loss: 0.1584 - val_acc: 0.8175\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1689 - acc: 0.7945 - val_loss: 0.1580 - val_acc: 0.8254\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1685 - acc: 0.7945 - val_loss: 0.1576 - val_acc: 0.8254\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1684 - acc: 0.7945 - val_loss: 0.1572 - val_acc: 0.8254\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1679 - acc: 0.7979 - val_loss: 0.1569 - val_acc: 0.8175\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1674 - acc: 0.8014 - val_loss: 0.1565 - val_acc: 0.8175\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1671 - acc: 0.8014 - val_loss: 0.1561 - val_acc: 0.8175\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1668 - acc: 0.8014 - val_loss: 0.1557 - val_acc: 0.8175\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1666 - acc: 0.8031 - val_loss: 0.1554 - val_acc: 0.8175\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1661 - acc: 0.8031 - val_loss: 0.1549 - val_acc: 0.8175\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1658 - acc: 0.7997 - val_loss: 0.1545 - val_acc: 0.8175\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1655 - acc: 0.8014 - val_loss: 0.1542 - val_acc: 0.8175\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1651 - acc: 0.7997 - val_loss: 0.1538 - val_acc: 0.8175\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1648 - acc: 0.7962 - val_loss: 0.1534 - val_acc: 0.8254\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1644 - acc: 0.7945 - val_loss: 0.1531 - val_acc: 0.8175\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1641 - acc: 0.7979 - val_loss: 0.1527 - val_acc: 0.8175\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1638 - acc: 0.7979 - val_loss: 0.1524 - val_acc: 0.8175\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1636 - acc: 0.7962 - val_loss: 0.1520 - val_acc: 0.8175\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1631 - acc: 0.7945 - val_loss: 0.1516 - val_acc: 0.8175\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1628 - acc: 0.7945 - val_loss: 0.1513 - val_acc: 0.8175\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1625 - acc: 0.7962 - val_loss: 0.1510 - val_acc: 0.8175\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1624 - acc: 0.7945 - val_loss: 0.1506 - val_acc: 0.8254\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1620 - acc: 0.7979 - val_loss: 0.1503 - val_acc: 0.8175\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 56us/step - loss: 0.1617 - acc: 0.7945 - val_loss: 0.1500 - val_acc: 0.8175\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1614 - acc: 0.7962 - val_loss: 0.1497 - val_acc: 0.8175\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1611 - acc: 0.7945 - val_loss: 0.1493 - val_acc: 0.8175\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1608 - acc: 0.7997 - val_loss: 0.1490 - val_acc: 0.8175\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1605 - acc: 0.7962 - val_loss: 0.1488 - val_acc: 0.8175\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1603 - acc: 0.7997 - val_loss: 0.1484 - val_acc: 0.8175\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1599 - acc: 0.7945 - val_loss: 0.1481 - val_acc: 0.8175\n",
      "Epoch 131/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1597 - acc: 0.7945 - val_loss: 0.1478 - val_acc: 0.8254\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1594 - acc: 0.7945 - val_loss: 0.1475 - val_acc: 0.8254\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1593 - acc: 0.7945 - val_loss: 0.1472 - val_acc: 0.8254\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1589 - acc: 0.7945 - val_loss: 0.1469 - val_acc: 0.8254\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1586 - acc: 0.7945 - val_loss: 0.1466 - val_acc: 0.8254\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1585 - acc: 0.7979 - val_loss: 0.1464 - val_acc: 0.8175\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1582 - acc: 0.7945 - val_loss: 0.1461 - val_acc: 0.8254\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1579 - acc: 0.7945 - val_loss: 0.1458 - val_acc: 0.8254\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1577 - acc: 0.7945 - val_loss: 0.1456 - val_acc: 0.8254\n",
      "Epoch 140/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1574 - acc: 0.7945 - val_loss: 0.1453 - val_acc: 0.8254\n",
      "Epoch 141/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1572 - acc: 0.7945 - val_loss: 0.1451 - val_acc: 0.8254\n",
      "Epoch 142/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1570 - acc: 0.7945 - val_loss: 0.1448 - val_acc: 0.8254\n",
      "Epoch 143/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1570 - acc: 0.7945 - val_loss: 0.1445 - val_acc: 0.8254\n",
      "Epoch 144/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1566 - acc: 0.7945 - val_loss: 0.1444 - val_acc: 0.8254\n",
      "Epoch 145/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1563 - acc: 0.7945 - val_loss: 0.1441 - val_acc: 0.8254\n",
      "Epoch 146/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1561 - acc: 0.7945 - val_loss: 0.1439 - val_acc: 0.8175\n",
      "Epoch 147/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1559 - acc: 0.7945 - val_loss: 0.1437 - val_acc: 0.8254\n",
      "Epoch 148/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1557 - acc: 0.7945 - val_loss: 0.1434 - val_acc: 0.8254\n",
      "Epoch 149/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1555 - acc: 0.7945 - val_loss: 0.1432 - val_acc: 0.8254\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 58us/step - loss: 0.1553 - acc: 0.7945 - val_loss: 0.1431 - val_acc: 0.8254\n",
      "Epoch 151/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1551 - acc: 0.7945 - val_loss: 0.1429 - val_acc: 0.8254\n",
      "Epoch 152/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1549 - acc: 0.7945 - val_loss: 0.1426 - val_acc: 0.8254\n",
      "Epoch 153/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1548 - acc: 0.7945 - val_loss: 0.1424 - val_acc: 0.8254\n",
      "Epoch 154/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1545 - acc: 0.7945 - val_loss: 0.1423 - val_acc: 0.8254\n",
      "Epoch 155/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1544 - acc: 0.7945 - val_loss: 0.1421 - val_acc: 0.8254\n",
      "Epoch 156/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1542 - acc: 0.7945 - val_loss: 0.1418 - val_acc: 0.8254\n",
      "Epoch 157/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1540 - acc: 0.7945 - val_loss: 0.1417 - val_acc: 0.8254\n",
      "Epoch 158/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1538 - acc: 0.7945 - val_loss: 0.1415 - val_acc: 0.8254\n",
      "Epoch 159/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1536 - acc: 0.7945 - val_loss: 0.1414 - val_acc: 0.8254\n",
      "Epoch 160/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1536 - acc: 0.7945 - val_loss: 0.1411 - val_acc: 0.8254\n",
      "Epoch 161/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1533 - acc: 0.7945 - val_loss: 0.1410 - val_acc: 0.8254\n",
      "Epoch 162/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1531 - acc: 0.7945 - val_loss: 0.1409 - val_acc: 0.8254\n",
      "Epoch 163/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1530 - acc: 0.7945 - val_loss: 0.1407 - val_acc: 0.8254\n",
      "Epoch 164/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1528 - acc: 0.7945 - val_loss: 0.1405 - val_acc: 0.8254\n",
      "Epoch 165/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1527 - acc: 0.7945 - val_loss: 0.1404 - val_acc: 0.8254\n",
      "Epoch 166/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1526 - acc: 0.7945 - val_loss: 0.1402 - val_acc: 0.8254\n",
      "Epoch 167/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1524 - acc: 0.7945 - val_loss: 0.1401 - val_acc: 0.8254\n",
      "Epoch 168/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1523 - acc: 0.7945 - val_loss: 0.1399 - val_acc: 0.8254\n",
      "Epoch 169/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1521 - acc: 0.7945 - val_loss: 0.1397 - val_acc: 0.8254\n",
      "Epoch 170/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1521 - acc: 0.7945 - val_loss: 0.1397 - val_acc: 0.8254\n",
      "Epoch 171/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1518 - acc: 0.7945 - val_loss: 0.1395 - val_acc: 0.8254\n",
      "Epoch 172/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1517 - acc: 0.7945 - val_loss: 0.1394 - val_acc: 0.8254\n",
      "Epoch 173/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1516 - acc: 0.7945 - val_loss: 0.1393 - val_acc: 0.8254\n",
      "Epoch 174/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1515 - acc: 0.7945 - val_loss: 0.1391 - val_acc: 0.8254\n",
      "Epoch 175/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1513 - acc: 0.7945 - val_loss: 0.1390 - val_acc: 0.8254\n",
      "Epoch 176/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1512 - acc: 0.7945 - val_loss: 0.1389 - val_acc: 0.8254\n",
      "Epoch 177/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1512 - acc: 0.7945 - val_loss: 0.1387 - val_acc: 0.8254\n",
      "Epoch 178/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1511 - acc: 0.7945 - val_loss: 0.1387 - val_acc: 0.8254\n",
      "Epoch 179/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1509 - acc: 0.7945 - val_loss: 0.1385 - val_acc: 0.8254\n",
      "Epoch 180/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1508 - acc: 0.7945 - val_loss: 0.1384 - val_acc: 0.8254\n",
      "Epoch 181/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1507 - acc: 0.7945 - val_loss: 0.1384 - val_acc: 0.8254\n",
      "Epoch 182/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1505 - acc: 0.7945 - val_loss: 0.1383 - val_acc: 0.8254\n",
      "Epoch 183/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1505 - acc: 0.7945 - val_loss: 0.1381 - val_acc: 0.8254\n",
      "Epoch 184/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1503 - acc: 0.7945 - val_loss: 0.1380 - val_acc: 0.8254\n",
      "Epoch 185/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1502 - acc: 0.7945 - val_loss: 0.1379 - val_acc: 0.8254\n",
      "Epoch 186/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1502 - acc: 0.7945 - val_loss: 0.1378 - val_acc: 0.8254\n",
      "Epoch 187/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1501 - acc: 0.7945 - val_loss: 0.1377 - val_acc: 0.8254\n",
      "Epoch 188/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1499 - acc: 0.7945 - val_loss: 0.1377 - val_acc: 0.8254\n",
      "Epoch 189/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1500 - acc: 0.7945 - val_loss: 0.1378 - val_acc: 0.8254\n",
      "Epoch 190/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1498 - acc: 0.7945 - val_loss: 0.1375 - val_acc: 0.8254\n",
      "Epoch 191/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1498 - acc: 0.7945 - val_loss: 0.1374 - val_acc: 0.8254\n",
      "Epoch 192/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1495 - acc: 0.7945 - val_loss: 0.1373 - val_acc: 0.8254\n",
      "Epoch 193/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1495 - acc: 0.7945 - val_loss: 0.1373 - val_acc: 0.8254\n",
      "Epoch 194/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1493 - acc: 0.7945 - val_loss: 0.1372 - val_acc: 0.8254\n",
      "Epoch 195/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1493 - acc: 0.7945 - val_loss: 0.1371 - val_acc: 0.8254\n",
      "Epoch 196/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1493 - acc: 0.7945 - val_loss: 0.1371 - val_acc: 0.8254\n",
      "Epoch 197/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1492 - acc: 0.7945 - val_loss: 0.1369 - val_acc: 0.8254\n",
      "Epoch 198/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1491 - acc: 0.7945 - val_loss: 0.1369 - val_acc: 0.8254\n",
      "Epoch 199/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1489 - acc: 0.7945 - val_loss: 0.1368 - val_acc: 0.8254\n",
      "Epoch 200/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1489 - acc: 0.7945 - val_loss: 0.1366 - val_acc: 0.8254\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 923us/step - loss: 1.6838 - acc: 0.0000e+00 - val_loss: 1.4234 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 1.2780 - acc: 0.1010 - val_loss: 1.0717 - val_acc: 0.2540\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.9652 - acc: 0.4384 - val_loss: 0.8067 - val_acc: 0.5317\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.7341 - acc: 0.5154 - val_loss: 0.6133 - val_acc: 0.5317\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.5639 - acc: 0.5154 - val_loss: 0.4771 - val_acc: 0.5317\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.4450 - acc: 0.5154 - val_loss: 0.3788 - val_acc: 0.5317\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.3591 - acc: 0.5154 - val_loss: 0.3114 - val_acc: 0.5317\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.3021 - acc: 0.5154 - val_loss: 0.2644 - val_acc: 0.5317\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.2622 - acc: 0.5154 - val_loss: 0.2352 - val_acc: 0.5317\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 63us/step - loss: 0.2379 - acc: 0.5154 - val_loss: 0.2153 - val_acc: 0.5317\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2217 - acc: 0.5154 - val_loss: 0.2033 - val_acc: 0.5397\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.2115 - acc: 0.5445 - val_loss: 0.1967 - val_acc: 0.7222\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2061 - acc: 0.7346 - val_loss: 0.1921 - val_acc: 0.8175\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2021 - acc: 0.7637 - val_loss: 0.1893 - val_acc: 0.8175\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1998 - acc: 0.7620 - val_loss: 0.1876 - val_acc: 0.8095\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1984 - acc: 0.7705 - val_loss: 0.1862 - val_acc: 0.8016\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1971 - acc: 0.7757 - val_loss: 0.1852 - val_acc: 0.8254\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1963 - acc: 0.7757 - val_loss: 0.1843 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1956 - acc: 0.7877 - val_loss: 0.1836 - val_acc: 0.8175\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1948 - acc: 0.7860 - val_loss: 0.1829 - val_acc: 0.8175\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1942 - acc: 0.7877 - val_loss: 0.1821 - val_acc: 0.8095\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1936 - acc: 0.7860 - val_loss: 0.1814 - val_acc: 0.8095\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1929 - acc: 0.7860 - val_loss: 0.1807 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1923 - acc: 0.7860 - val_loss: 0.1800 - val_acc: 0.8095\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1916 - acc: 0.7877 - val_loss: 0.1793 - val_acc: 0.8095\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1910 - acc: 0.7860 - val_loss: 0.1785 - val_acc: 0.8095\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1903 - acc: 0.7894 - val_loss: 0.1778 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1897 - acc: 0.7894 - val_loss: 0.1771 - val_acc: 0.8095\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1892 - acc: 0.7877 - val_loss: 0.1764 - val_acc: 0.8095\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1885 - acc: 0.7894 - val_loss: 0.1757 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1878 - acc: 0.7894 - val_loss: 0.1750 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1872 - acc: 0.7894 - val_loss: 0.1743 - val_acc: 0.8095\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1866 - acc: 0.7894 - val_loss: 0.1736 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1860 - acc: 0.7894 - val_loss: 0.1730 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1855 - acc: 0.7894 - val_loss: 0.1723 - val_acc: 0.8095\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1849 - acc: 0.7894 - val_loss: 0.1716 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1842 - acc: 0.7894 - val_loss: 0.1710 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1836 - acc: 0.7894 - val_loss: 0.1703 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1830 - acc: 0.7894 - val_loss: 0.1697 - val_acc: 0.8095\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1824 - acc: 0.7894 - val_loss: 0.1690 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1819 - acc: 0.7877 - val_loss: 0.1684 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1812 - acc: 0.7894 - val_loss: 0.1677 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1807 - acc: 0.7877 - val_loss: 0.1671 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1801 - acc: 0.7877 - val_loss: 0.1665 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1796 - acc: 0.7877 - val_loss: 0.1659 - val_acc: 0.8095\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1790 - acc: 0.7877 - val_loss: 0.1653 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1785 - acc: 0.7877 - val_loss: 0.1647 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1780 - acc: 0.7877 - val_loss: 0.1641 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1774 - acc: 0.7877 - val_loss: 0.1635 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1769 - acc: 0.7877 - val_loss: 0.1630 - val_acc: 0.8095\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1764 - acc: 0.7877 - val_loss: 0.1624 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1759 - acc: 0.7877 - val_loss: 0.1619 - val_acc: 0.8095\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1754 - acc: 0.7877 - val_loss: 0.1613 - val_acc: 0.8095\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1749 - acc: 0.7877 - val_loss: 0.1608 - val_acc: 0.8095\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1744 - acc: 0.7877 - val_loss: 0.1603 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1741 - acc: 0.7877 - val_loss: 0.1598 - val_acc: 0.8095\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1735 - acc: 0.7877 - val_loss: 0.1593 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1730 - acc: 0.7877 - val_loss: 0.1588 - val_acc: 0.8095\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1726 - acc: 0.7894 - val_loss: 0.1583 - val_acc: 0.8095\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1721 - acc: 0.7894 - val_loss: 0.1578 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1717 - acc: 0.7911 - val_loss: 0.1573 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1712 - acc: 0.7894 - val_loss: 0.1568 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1708 - acc: 0.7894 - val_loss: 0.1564 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1704 - acc: 0.7894 - val_loss: 0.1560 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1700 - acc: 0.7894 - val_loss: 0.1555 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1696 - acc: 0.7911 - val_loss: 0.1551 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1693 - acc: 0.7911 - val_loss: 0.1547 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1688 - acc: 0.7894 - val_loss: 0.1543 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1685 - acc: 0.7911 - val_loss: 0.1538 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1680 - acc: 0.7894 - val_loss: 0.1535 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1678 - acc: 0.7894 - val_loss: 0.1532 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1673 - acc: 0.7894 - val_loss: 0.1528 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1670 - acc: 0.7894 - val_loss: 0.1524 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1667 - acc: 0.7877 - val_loss: 0.1520 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1663 - acc: 0.7877 - val_loss: 0.1517 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1660 - acc: 0.7877 - val_loss: 0.1513 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1656 - acc: 0.7894 - val_loss: 0.1509 - val_acc: 0.8095\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1653 - acc: 0.7894 - val_loss: 0.1506 - val_acc: 0.8095\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1650 - acc: 0.7894 - val_loss: 0.1503 - val_acc: 0.8095\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1647 - acc: 0.7894 - val_loss: 0.1499 - val_acc: 0.8095\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1644 - acc: 0.7877 - val_loss: 0.1497 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1641 - acc: 0.7877 - val_loss: 0.1493 - val_acc: 0.8095\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1638 - acc: 0.7894 - val_loss: 0.1490 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1635 - acc: 0.7894 - val_loss: 0.1487 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1632 - acc: 0.7894 - val_loss: 0.1485 - val_acc: 0.8095\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1630 - acc: 0.7894 - val_loss: 0.1482 - val_acc: 0.8095\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1626 - acc: 0.7894 - val_loss: 0.1479 - val_acc: 0.8095\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1624 - acc: 0.7894 - val_loss: 0.1476 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1621 - acc: 0.7894 - val_loss: 0.1473 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1619 - acc: 0.7894 - val_loss: 0.1471 - val_acc: 0.8095\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1616 - acc: 0.7894 - val_loss: 0.1469 - val_acc: 0.8095\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1614 - acc: 0.7894 - val_loss: 0.1466 - val_acc: 0.8095\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1612 - acc: 0.7894 - val_loss: 0.1464 - val_acc: 0.8095\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1609 - acc: 0.7894 - val_loss: 0.1462 - val_acc: 0.8095\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1607 - acc: 0.7877 - val_loss: 0.1459 - val_acc: 0.8095\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1605 - acc: 0.7911 - val_loss: 0.1456 - val_acc: 0.8095\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 108us/step - loss: 0.1602 - acc: 0.7911 - val_loss: 0.1454 - val_acc: 0.8095\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1600 - acc: 0.7894 - val_loss: 0.1452 - val_acc: 0.8095\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1598 - acc: 0.7894 - val_loss: 0.1450 - val_acc: 0.8095\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1597 - acc: 0.7911 - val_loss: 0.1448 - val_acc: 0.8095\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1593 - acc: 0.7894 - val_loss: 0.1447 - val_acc: 0.8095\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1591 - acc: 0.7894 - val_loss: 0.1445 - val_acc: 0.8095\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1590 - acc: 0.7894 - val_loss: 0.1443 - val_acc: 0.8095\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1588 - acc: 0.7911 - val_loss: 0.1441 - val_acc: 0.8095\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1587 - acc: 0.7928 - val_loss: 0.1438 - val_acc: 0.8095\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1584 - acc: 0.7894 - val_loss: 0.1437 - val_acc: 0.8095\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1582 - acc: 0.7894 - val_loss: 0.1435 - val_acc: 0.8095\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1581 - acc: 0.7911 - val_loss: 0.1433 - val_acc: 0.8095\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1579 - acc: 0.7911 - val_loss: 0.1432 - val_acc: 0.8095\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1577 - acc: 0.7894 - val_loss: 0.1431 - val_acc: 0.8095\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1575 - acc: 0.7894 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1573 - acc: 0.7928 - val_loss: 0.1427 - val_acc: 0.8095\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1572 - acc: 0.7911 - val_loss: 0.1426 - val_acc: 0.8095\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1571 - acc: 0.7911 - val_loss: 0.1423 - val_acc: 0.8095\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1568 - acc: 0.7928 - val_loss: 0.1422 - val_acc: 0.8095\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1567 - acc: 0.7911 - val_loss: 0.1421 - val_acc: 0.8095\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1566 - acc: 0.7928 - val_loss: 0.1420 - val_acc: 0.8095\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1564 - acc: 0.7894 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1563 - acc: 0.7945 - val_loss: 0.1417 - val_acc: 0.8095\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1563 - acc: 0.7911 - val_loss: 0.1417 - val_acc: 0.8095\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1560 - acc: 0.7928 - val_loss: 0.1415 - val_acc: 0.8095\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1558 - acc: 0.7945 - val_loss: 0.1413 - val_acc: 0.8095\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1557 - acc: 0.7945 - val_loss: 0.1412 - val_acc: 0.8095\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1558 - acc: 0.7945 - val_loss: 0.1413 - val_acc: 0.8095\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1555 - acc: 0.7945 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1553 - acc: 0.7945 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1553 - acc: 0.7945 - val_loss: 0.1408 - val_acc: 0.8095\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1550 - acc: 0.7945 - val_loss: 0.1407 - val_acc: 0.8095\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1549 - acc: 0.7945 - val_loss: 0.1407 - val_acc: 0.8095\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1549 - acc: 0.7911 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 74us/step - loss: 0.1547 - acc: 0.7945 - val_loss: 0.1404 - val_acc: 0.8095\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1545 - acc: 0.7945 - val_loss: 0.1404 - val_acc: 0.8095\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1544 - acc: 0.7945 - val_loss: 0.1403 - val_acc: 0.8095\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1543 - acc: 0.7945 - val_loss: 0.1401 - val_acc: 0.8095\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1543 - acc: 0.7945 - val_loss: 0.1401 - val_acc: 0.8095\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1541 - acc: 0.7945 - val_loss: 0.1399 - val_acc: 0.8095\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1540 - acc: 0.7945 - val_loss: 0.1399 - val_acc: 0.8095\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1540 - acc: 0.7979 - val_loss: 0.1397 - val_acc: 0.8095\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1538 - acc: 0.7979 - val_loss: 0.1396 - val_acc: 0.8095\n",
      "Epoch 140/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1538 - acc: 0.7962 - val_loss: 0.1396 - val_acc: 0.8095\n",
      "Epoch 141/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1537 - acc: 0.7945 - val_loss: 0.1398 - val_acc: 0.8095\n",
      "Epoch 142/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1535 - acc: 0.7945 - val_loss: 0.1394 - val_acc: 0.8095\n",
      "Epoch 143/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1534 - acc: 0.7945 - val_loss: 0.1395 - val_acc: 0.8095\n",
      "Epoch 144/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1533 - acc: 0.7962 - val_loss: 0.1393 - val_acc: 0.8095\n",
      "Epoch 145/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1532 - acc: 0.7945 - val_loss: 0.1393 - val_acc: 0.8095\n",
      "Epoch 146/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1533 - acc: 0.7945 - val_loss: 0.1393 - val_acc: 0.8095\n",
      "Epoch 147/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1530 - acc: 0.7945 - val_loss: 0.1391 - val_acc: 0.8095\n",
      "Epoch 148/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1529 - acc: 0.7979 - val_loss: 0.1391 - val_acc: 0.8095\n",
      "Epoch 149/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1529 - acc: 0.7979 - val_loss: 0.1389 - val_acc: 0.8095\n",
      "Epoch 150/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1527 - acc: 0.7997 - val_loss: 0.1389 - val_acc: 0.8095\n",
      "Epoch 151/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1526 - acc: 0.7997 - val_loss: 0.1388 - val_acc: 0.8095\n",
      "Epoch 152/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1525 - acc: 0.7997 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 153/200\n",
      "584/584 [==============================] - 0s 129us/step - loss: 0.1524 - acc: 0.7979 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 154/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1524 - acc: 0.7997 - val_loss: 0.1385 - val_acc: 0.8095\n",
      "Epoch 155/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1524 - acc: 0.7962 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 156/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1522 - acc: 0.7979 - val_loss: 0.1386 - val_acc: 0.8095\n",
      "Epoch 157/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1521 - acc: 0.7979 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 158/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1520 - acc: 0.7997 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 159/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1520 - acc: 0.7997 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 160/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1518 - acc: 0.7997 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 161/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1518 - acc: 0.7997 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 162/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1517 - acc: 0.7997 - val_loss: 0.1382 - val_acc: 0.8095\n",
      "Epoch 163/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1517 - acc: 0.7979 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 164/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1516 - acc: 0.7979 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 165/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1514 - acc: 0.7997 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 166/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1514 - acc: 0.7979 - val_loss: 0.1379 - val_acc: 0.8095\n",
      "Epoch 167/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1514 - acc: 0.7962 - val_loss: 0.1377 - val_acc: 0.8095\n",
      "Epoch 168/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1512 - acc: 0.7979 - val_loss: 0.1378 - val_acc: 0.8095\n",
      "Epoch 169/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1511 - acc: 0.7979 - val_loss: 0.1377 - val_acc: 0.8095\n",
      "Epoch 170/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1513 - acc: 0.7979 - val_loss: 0.1378 - val_acc: 0.8095\n",
      "Epoch 171/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1510 - acc: 0.7997 - val_loss: 0.1377 - val_acc: 0.8095\n",
      "Epoch 172/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1509 - acc: 0.7962 - val_loss: 0.1376 - val_acc: 0.8095\n",
      "Epoch 173/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1509 - acc: 0.7979 - val_loss: 0.1375 - val_acc: 0.8095\n",
      "Epoch 174/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1509 - acc: 0.7962 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 175/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1507 - acc: 0.7979 - val_loss: 0.1375 - val_acc: 0.8095\n",
      "Epoch 176/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1508 - acc: 0.7962 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 177/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1505 - acc: 0.7979 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 178/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1505 - acc: 0.7979 - val_loss: 0.1375 - val_acc: 0.8095\n",
      "Epoch 179/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1504 - acc: 0.7979 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 180/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1504 - acc: 0.7997 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 181/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1503 - acc: 0.7979 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 182/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1502 - acc: 0.7962 - val_loss: 0.1371 - val_acc: 0.8095\n",
      "Epoch 183/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1503 - acc: 0.7997 - val_loss: 0.1372 - val_acc: 0.8095\n",
      "Epoch 184/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1501 - acc: 0.7979 - val_loss: 0.1371 - val_acc: 0.8095\n",
      "Epoch 185/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1500 - acc: 0.7962 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 186/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1500 - acc: 0.7979 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 187/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1500 - acc: 0.7962 - val_loss: 0.1368 - val_acc: 0.8095\n",
      "Epoch 188/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1498 - acc: 0.7979 - val_loss: 0.1368 - val_acc: 0.8095\n",
      "Epoch 189/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1498 - acc: 0.7979 - val_loss: 0.1368 - val_acc: 0.8095\n",
      "Epoch 190/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1497 - acc: 0.7962 - val_loss: 0.1368 - val_acc: 0.8095\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 61us/step - loss: 0.1497 - acc: 0.7979 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 192/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1496 - acc: 0.7979 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 193/200\n",
      "584/584 [==============================] - 0s 57us/step - loss: 0.1497 - acc: 0.7979 - val_loss: 0.1369 - val_acc: 0.8095\n",
      "Epoch 194/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1495 - acc: 0.7962 - val_loss: 0.1366 - val_acc: 0.8095\n",
      "Epoch 00194: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.7057 - acc: 0.5154 - val_loss: 0.5009 - val_acc: 0.5317\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.4094 - acc: 0.5154 - val_loss: 0.2983 - val_acc: 0.5317\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.2672 - acc: 0.5154 - val_loss: 0.2163 - val_acc: 0.5317\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.2157 - acc: 0.6027 - val_loss: 0.1931 - val_acc: 0.8175\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.2029 - acc: 0.7551 - val_loss: 0.1883 - val_acc: 0.8175\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.2000 - acc: 0.7586 - val_loss: 0.1867 - val_acc: 0.8254\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1989 - acc: 0.7637 - val_loss: 0.1854 - val_acc: 0.8254\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1977 - acc: 0.7671 - val_loss: 0.1841 - val_acc: 0.8254\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1964 - acc: 0.7688 - val_loss: 0.1828 - val_acc: 0.8254\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1953 - acc: 0.7654 - val_loss: 0.1813 - val_acc: 0.8254\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1940 - acc: 0.7688 - val_loss: 0.1800 - val_acc: 0.8254\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1928 - acc: 0.7723 - val_loss: 0.1786 - val_acc: 0.8254\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1916 - acc: 0.7723 - val_loss: 0.1772 - val_acc: 0.8254\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1906 - acc: 0.7740 - val_loss: 0.1759 - val_acc: 0.8254\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1891 - acc: 0.7723 - val_loss: 0.1744 - val_acc: 0.8254\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1881 - acc: 0.7757 - val_loss: 0.1732 - val_acc: 0.8254\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1867 - acc: 0.7791 - val_loss: 0.1718 - val_acc: 0.8254\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1856 - acc: 0.7791 - val_loss: 0.1705 - val_acc: 0.8254\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1844 - acc: 0.7808 - val_loss: 0.1692 - val_acc: 0.8254\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1833 - acc: 0.7825 - val_loss: 0.1680 - val_acc: 0.8254\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1822 - acc: 0.7860 - val_loss: 0.1667 - val_acc: 0.8254\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1810 - acc: 0.7842 - val_loss: 0.1656 - val_acc: 0.8254\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1800 - acc: 0.7860 - val_loss: 0.1644 - val_acc: 0.8254\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1789 - acc: 0.7860 - val_loss: 0.1633 - val_acc: 0.8254\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1779 - acc: 0.7860 - val_loss: 0.1622 - val_acc: 0.8254\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1773 - acc: 0.7877 - val_loss: 0.1613 - val_acc: 0.8095\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1760 - acc: 0.7894 - val_loss: 0.1602 - val_acc: 0.8254\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1751 - acc: 0.7860 - val_loss: 0.1592 - val_acc: 0.8254\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1742 - acc: 0.7877 - val_loss: 0.1582 - val_acc: 0.8254\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1733 - acc: 0.7877 - val_loss: 0.1573 - val_acc: 0.8175\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1725 - acc: 0.7877 - val_loss: 0.1565 - val_acc: 0.8254\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1717 - acc: 0.7877 - val_loss: 0.1557 - val_acc: 0.8254\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 132us/step - loss: 0.1710 - acc: 0.7877 - val_loss: 0.1549 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1701 - acc: 0.7877 - val_loss: 0.1541 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1693 - acc: 0.7877 - val_loss: 0.1534 - val_acc: 0.8095\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1689 - acc: 0.7877 - val_loss: 0.1526 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1680 - acc: 0.7877 - val_loss: 0.1520 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1674 - acc: 0.7877 - val_loss: 0.1513 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1667 - acc: 0.7877 - val_loss: 0.1506 - val_acc: 0.8095\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1664 - acc: 0.7877 - val_loss: 0.1501 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1657 - acc: 0.7877 - val_loss: 0.1496 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1651 - acc: 0.7877 - val_loss: 0.1489 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1645 - acc: 0.7877 - val_loss: 0.1483 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1641 - acc: 0.7894 - val_loss: 0.1480 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1634 - acc: 0.7928 - val_loss: 0.1473 - val_acc: 0.8095\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 108us/step - loss: 0.1630 - acc: 0.7894 - val_loss: 0.1470 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1624 - acc: 0.7877 - val_loss: 0.1465 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1624 - acc: 0.7945 - val_loss: 0.1460 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1616 - acc: 0.7894 - val_loss: 0.1458 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1612 - acc: 0.7894 - val_loss: 0.1453 - val_acc: 0.8095\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1607 - acc: 0.7928 - val_loss: 0.1449 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1606 - acc: 0.7945 - val_loss: 0.1445 - val_acc: 0.8095\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1600 - acc: 0.7928 - val_loss: 0.1443 - val_acc: 0.8095\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1597 - acc: 0.7911 - val_loss: 0.1439 - val_acc: 0.8095\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1593 - acc: 0.7928 - val_loss: 0.1435 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1591 - acc: 0.7911 - val_loss: 0.1436 - val_acc: 0.8095\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 76us/step - loss: 0.1590 - acc: 0.7911 - val_loss: 0.1430 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1584 - acc: 0.7911 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1581 - acc: 0.7911 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1579 - acc: 0.7928 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1573 - acc: 0.7911 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1573 - acc: 0.7928 - val_loss: 0.1417 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1569 - acc: 0.7945 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1570 - acc: 0.7911 - val_loss: 0.1414 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1566 - acc: 0.7894 - val_loss: 0.1417 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1565 - acc: 0.7911 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1559 - acc: 0.7945 - val_loss: 0.1408 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1557 - acc: 0.7928 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1556 - acc: 0.7928 - val_loss: 0.1403 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1553 - acc: 0.7945 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1552 - acc: 0.7911 - val_loss: 0.1402 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1549 - acc: 0.7945 - val_loss: 0.1401 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1547 - acc: 0.7928 - val_loss: 0.1398 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1545 - acc: 0.7928 - val_loss: 0.1396 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1543 - acc: 0.7928 - val_loss: 0.1396 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1540 - acc: 0.7945 - val_loss: 0.1393 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1540 - acc: 0.7928 - val_loss: 0.1394 - val_acc: 0.8095\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1537 - acc: 0.7945 - val_loss: 0.1390 - val_acc: 0.8095\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1535 - acc: 0.7928 - val_loss: 0.1392 - val_acc: 0.8095\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1536 - acc: 0.7911 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1531 - acc: 0.7945 - val_loss: 0.1392 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1530 - acc: 0.7928 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1528 - acc: 0.7928 - val_loss: 0.1385 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1527 - acc: 0.7928 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1525 - acc: 0.7928 - val_loss: 0.1381 - val_acc: 0.8095\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1524 - acc: 0.7911 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1521 - acc: 0.7911 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1520 - acc: 0.7945 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1521 - acc: 0.7945 - val_loss: 0.1379 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1519 - acc: 0.7911 - val_loss: 0.1379 - val_acc: 0.8095\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1516 - acc: 0.7911 - val_loss: 0.1379 - val_acc: 0.8095\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1513 - acc: 0.7928 - val_loss: 0.1376 - val_acc: 0.8095\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1513 - acc: 0.7928 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1513 - acc: 0.7928 - val_loss: 0.1378 - val_acc: 0.8095\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1510 - acc: 0.7945 - val_loss: 0.1372 - val_acc: 0.8095\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1509 - acc: 0.7945 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1507 - acc: 0.7945 - val_loss: 0.1372 - val_acc: 0.8095\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1507 - acc: 0.7962 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1505 - acc: 0.7945 - val_loss: 0.1372 - val_acc: 0.8095\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1506 - acc: 0.7962 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1503 - acc: 0.7945 - val_loss: 0.1369 - val_acc: 0.8095\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1503 - acc: 0.7979 - val_loss: 0.1371 - val_acc: 0.8095\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1504 - acc: 0.7962 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1498 - acc: 0.7945 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1496 - acc: 0.7945 - val_loss: 0.1366 - val_acc: 0.8095\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1495 - acc: 0.7962 - val_loss: 0.1364 - val_acc: 0.8095\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1494 - acc: 0.7979 - val_loss: 0.1364 - val_acc: 0.8095\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1493 - acc: 0.7979 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1494 - acc: 0.7979 - val_loss: 0.1363 - val_acc: 0.8095\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1492 - acc: 0.7945 - val_loss: 0.1363 - val_acc: 0.8095\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1489 - acc: 0.7979 - val_loss: 0.1362 - val_acc: 0.8095\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1489 - acc: 0.7979 - val_loss: 0.1361 - val_acc: 0.8095\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1488 - acc: 0.7979 - val_loss: 0.1360 - val_acc: 0.8095\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1487 - acc: 0.7979 - val_loss: 0.1360 - val_acc: 0.8095\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1485 - acc: 0.8014 - val_loss: 0.1360 - val_acc: 0.8095\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1486 - acc: 0.7979 - val_loss: 0.1362 - val_acc: 0.8095\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 104us/step - loss: 0.1483 - acc: 0.7962 - val_loss: 0.1357 - val_acc: 0.8095\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1482 - acc: 0.8014 - val_loss: 0.1358 - val_acc: 0.8095\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1481 - acc: 0.7997 - val_loss: 0.1356 - val_acc: 0.8095\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1480 - acc: 0.7979 - val_loss: 0.1354 - val_acc: 0.8095\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1478 - acc: 0.7997 - val_loss: 0.1357 - val_acc: 0.8095\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1479 - acc: 0.8014 - val_loss: 0.1355 - val_acc: 0.8095\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1477 - acc: 0.8031 - val_loss: 0.1355 - val_acc: 0.8095\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 129us/step - loss: 0.1476 - acc: 0.8031 - val_loss: 0.1354 - val_acc: 0.8095\n",
      "Epoch 125/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1475 - acc: 0.7997 - val_loss: 0.1352 - val_acc: 0.8095\n",
      "Epoch 126/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1473 - acc: 0.7979 - val_loss: 0.1353 - val_acc: 0.8095\n",
      "Epoch 127/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1474 - acc: 0.7997 - val_loss: 0.1358 - val_acc: 0.8095\n",
      "Epoch 128/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1472 - acc: 0.7979 - val_loss: 0.1352 - val_acc: 0.8095\n",
      "Epoch 129/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1476 - acc: 0.7962 - val_loss: 0.1349 - val_acc: 0.8095\n",
      "Epoch 130/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1470 - acc: 0.7979 - val_loss: 0.1352 - val_acc: 0.8095\n",
      "Epoch 131/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1472 - acc: 0.8014 - val_loss: 0.1352 - val_acc: 0.8095\n",
      "Epoch 132/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1474 - acc: 0.7997 - val_loss: 0.1347 - val_acc: 0.8095\n",
      "Epoch 133/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1467 - acc: 0.7962 - val_loss: 0.1350 - val_acc: 0.8095\n",
      "Epoch 134/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1467 - acc: 0.8014 - val_loss: 0.1353 - val_acc: 0.8095\n",
      "Epoch 135/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1471 - acc: 0.7979 - val_loss: 0.1346 - val_acc: 0.8095\n",
      "Epoch 136/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1466 - acc: 0.7962 - val_loss: 0.1351 - val_acc: 0.8095\n",
      "Epoch 137/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1465 - acc: 0.8014 - val_loss: 0.1346 - val_acc: 0.8095\n",
      "Epoch 138/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1465 - acc: 0.7997 - val_loss: 0.1347 - val_acc: 0.8095\n",
      "Epoch 139/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1462 - acc: 0.8014 - val_loss: 0.1348 - val_acc: 0.8095\n",
      "Epoch 140/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1461 - acc: 0.7979 - val_loss: 0.1347 - val_acc: 0.8095\n",
      "Epoch 141/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1461 - acc: 0.7979 - val_loss: 0.1345 - val_acc: 0.8095\n",
      "Epoch 142/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1462 - acc: 0.7997 - val_loss: 0.1346 - val_acc: 0.8095\n",
      "Epoch 143/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1460 - acc: 0.7979 - val_loss: 0.1344 - val_acc: 0.8095\n",
      "Epoch 144/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1458 - acc: 0.7979 - val_loss: 0.1344 - val_acc: 0.8095\n",
      "Epoch 145/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1458 - acc: 0.7979 - val_loss: 0.1345 - val_acc: 0.8095\n",
      "Epoch 146/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1456 - acc: 0.7979 - val_loss: 0.1342 - val_acc: 0.8095\n",
      "Epoch 147/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1457 - acc: 0.7962 - val_loss: 0.1341 - val_acc: 0.8095\n",
      "Epoch 148/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1456 - acc: 0.7979 - val_loss: 0.1343 - val_acc: 0.8095\n",
      "Epoch 149/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1456 - acc: 0.7979 - val_loss: 0.1341 - val_acc: 0.8095\n",
      "Epoch 150/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1454 - acc: 0.7997 - val_loss: 0.1343 - val_acc: 0.8095\n",
      "Epoch 151/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1453 - acc: 0.8014 - val_loss: 0.1343 - val_acc: 0.8095\n",
      "Epoch 152/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1455 - acc: 0.7997 - val_loss: 0.1340 - val_acc: 0.8095\n",
      "Epoch 153/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1453 - acc: 0.7997 - val_loss: 0.1340 - val_acc: 0.8095\n",
      "Epoch 154/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1453 - acc: 0.8031 - val_loss: 0.1344 - val_acc: 0.8095\n",
      "Epoch 155/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1450 - acc: 0.7997 - val_loss: 0.1339 - val_acc: 0.8095\n",
      "Epoch 00155: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 956us/step - loss: 0.3175 - acc: 0.4846 - val_loss: 0.2807 - val_acc: 0.4683\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2661 - acc: 0.4486 - val_loss: 0.2605 - val_acc: 0.1905\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.2583 - acc: 0.4041 - val_loss: 0.2526 - val_acc: 0.5079\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2519 - acc: 0.4983 - val_loss: 0.2455 - val_acc: 0.5714\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2466 - acc: 0.5068 - val_loss: 0.2388 - val_acc: 0.6905\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.2400 - acc: 0.6935 - val_loss: 0.2326 - val_acc: 0.7778\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.2343 - acc: 0.7380 - val_loss: 0.2263 - val_acc: 0.7857\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2291 - acc: 0.7517 - val_loss: 0.2206 - val_acc: 0.8016\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.2242 - acc: 0.7654 - val_loss: 0.2155 - val_acc: 0.8016\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2195 - acc: 0.7654 - val_loss: 0.2103 - val_acc: 0.8016\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.2152 - acc: 0.7791 - val_loss: 0.2054 - val_acc: 0.8016\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2114 - acc: 0.7757 - val_loss: 0.2010 - val_acc: 0.8095\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2072 - acc: 0.7808 - val_loss: 0.1966 - val_acc: 0.8016\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2042 - acc: 0.7740 - val_loss: 0.1930 - val_acc: 0.8095\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2002 - acc: 0.7705 - val_loss: 0.1890 - val_acc: 0.8016\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1971 - acc: 0.7808 - val_loss: 0.1853 - val_acc: 0.8016\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1941 - acc: 0.7808 - val_loss: 0.1821 - val_acc: 0.8016\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1910 - acc: 0.7842 - val_loss: 0.1789 - val_acc: 0.8016\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1886 - acc: 0.7774 - val_loss: 0.1762 - val_acc: 0.8095\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1866 - acc: 0.7791 - val_loss: 0.1732 - val_acc: 0.8016\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1837 - acc: 0.7791 - val_loss: 0.1707 - val_acc: 0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1811 - acc: 0.7808 - val_loss: 0.1683 - val_acc: 0.8095\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1790 - acc: 0.7808 - val_loss: 0.1661 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1771 - acc: 0.7808 - val_loss: 0.1642 - val_acc: 0.8016\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1754 - acc: 0.7825 - val_loss: 0.1621 - val_acc: 0.8095\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1735 - acc: 0.7825 - val_loss: 0.1603 - val_acc: 0.8016\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1722 - acc: 0.7791 - val_loss: 0.1587 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1709 - acc: 0.7928 - val_loss: 0.1575 - val_acc: 0.8016\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1695 - acc: 0.7911 - val_loss: 0.1557 - val_acc: 0.8095\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1680 - acc: 0.7808 - val_loss: 0.1545 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.7842 - val_loss: 0.1534 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1657 - acc: 0.7842 - val_loss: 0.1522 - val_acc: 0.8095\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1648 - acc: 0.7842 - val_loss: 0.1511 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1637 - acc: 0.7860 - val_loss: 0.1505 - val_acc: 0.8016\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1633 - acc: 0.7877 - val_loss: 0.1494 - val_acc: 0.8095\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1620 - acc: 0.7842 - val_loss: 0.1486 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1613 - acc: 0.7877 - val_loss: 0.1479 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1606 - acc: 0.7842 - val_loss: 0.1472 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1599 - acc: 0.7842 - val_loss: 0.1466 - val_acc: 0.8095\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1594 - acc: 0.7825 - val_loss: 0.1459 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1590 - acc: 0.7894 - val_loss: 0.1455 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1586 - acc: 0.7842 - val_loss: 0.1448 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1581 - acc: 0.7877 - val_loss: 0.1447 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1576 - acc: 0.7860 - val_loss: 0.1442 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1570 - acc: 0.7842 - val_loss: 0.1437 - val_acc: 0.8095\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1567 - acc: 0.7842 - val_loss: 0.1437 - val_acc: 0.8016\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1563 - acc: 0.7860 - val_loss: 0.1430 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1557 - acc: 0.7877 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1554 - acc: 0.7860 - val_loss: 0.1424 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1554 - acc: 0.7860 - val_loss: 0.1423 - val_acc: 0.8095\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1549 - acc: 0.7877 - val_loss: 0.1421 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1544 - acc: 0.7842 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1542 - acc: 0.7860 - val_loss: 0.1414 - val_acc: 0.8095\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1539 - acc: 0.7877 - val_loss: 0.1415 - val_acc: 0.8095\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1536 - acc: 0.7894 - val_loss: 0.1411 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1536 - acc: 0.7877 - val_loss: 0.1410 - val_acc: 0.8095\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1531 - acc: 0.7860 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1537 - acc: 0.7894 - val_loss: 0.1404 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1525 - acc: 0.7894 - val_loss: 0.1409 - val_acc: 0.8095\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1527 - acc: 0.7945 - val_loss: 0.1405 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1525 - acc: 0.7911 - val_loss: 0.1400 - val_acc: 0.8175\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1528 - acc: 0.7911 - val_loss: 0.1402 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1523 - acc: 0.7911 - val_loss: 0.1399 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1518 - acc: 0.7894 - val_loss: 0.1398 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1518 - acc: 0.7894 - val_loss: 0.1397 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1516 - acc: 0.7928 - val_loss: 0.1395 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1515 - acc: 0.7894 - val_loss: 0.1395 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1513 - acc: 0.7911 - val_loss: 0.1394 - val_acc: 0.8095\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1510 - acc: 0.7928 - val_loss: 0.1391 - val_acc: 0.8175\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1510 - acc: 0.7911 - val_loss: 0.1390 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1508 - acc: 0.7928 - val_loss: 0.1390 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1506 - acc: 0.7911 - val_loss: 0.1390 - val_acc: 0.8095\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1509 - acc: 0.7911 - val_loss: 0.1388 - val_acc: 0.8175\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1504 - acc: 0.7911 - val_loss: 0.1388 - val_acc: 0.8095\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1504 - acc: 0.7928 - val_loss: 0.1389 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1502 - acc: 0.7911 - val_loss: 0.1386 - val_acc: 0.8175\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1501 - acc: 0.7911 - val_loss: 0.1384 - val_acc: 0.8175\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1499 - acc: 0.7945 - val_loss: 0.1383 - val_acc: 0.8175\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1497 - acc: 0.7928 - val_loss: 0.1385 - val_acc: 0.8095\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1497 - acc: 0.7928 - val_loss: 0.1383 - val_acc: 0.8175\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1496 - acc: 0.7962 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1493 - acc: 0.7945 - val_loss: 0.1381 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1492 - acc: 0.7945 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1490 - acc: 0.7962 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.1493 - acc: 0.7945 - val_loss: 0.1378 - val_acc: 0.8175\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1491 - acc: 0.7979 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1494 - acc: 0.7997 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 65us/step - loss: 0.1489 - acc: 0.7962 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1484 - acc: 0.7928 - val_loss: 0.1375 - val_acc: 0.8175\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1484 - acc: 0.7945 - val_loss: 0.1375 - val_acc: 0.8175\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1483 - acc: 0.7997 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1487 - acc: 0.7979 - val_loss: 0.1375 - val_acc: 0.8175\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1481 - acc: 0.7997 - val_loss: 0.1377 - val_acc: 0.8095\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1484 - acc: 0.8031 - val_loss: 0.1374 - val_acc: 0.8175\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1479 - acc: 0.7962 - val_loss: 0.1372 - val_acc: 0.8175\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1478 - acc: 0.7962 - val_loss: 0.1372 - val_acc: 0.8175\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1478 - acc: 0.7945 - val_loss: 0.1373 - val_acc: 0.8175\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1476 - acc: 0.8031 - val_loss: 0.1375 - val_acc: 0.8095\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1478 - acc: 0.7997 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1475 - acc: 0.8014 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1473 - acc: 0.7979 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1473 - acc: 0.8014 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1471 - acc: 0.7997 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1471 - acc: 0.7962 - val_loss: 0.1367 - val_acc: 0.8175\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1471 - acc: 0.7997 - val_loss: 0.1370 - val_acc: 0.8175\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1470 - acc: 0.7979 - val_loss: 0.1365 - val_acc: 0.8175\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1467 - acc: 0.7979 - val_loss: 0.1366 - val_acc: 0.8175\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1467 - acc: 0.8031 - val_loss: 0.1367 - val_acc: 0.8175\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1467 - acc: 0.7979 - val_loss: 0.1364 - val_acc: 0.8175\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1466 - acc: 0.8048 - val_loss: 0.1367 - val_acc: 0.8175\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1465 - acc: 0.8048 - val_loss: 0.1366 - val_acc: 0.8175\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1464 - acc: 0.8014 - val_loss: 0.1364 - val_acc: 0.8175\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1464 - acc: 0.7962 - val_loss: 0.1362 - val_acc: 0.8175\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1462 - acc: 0.7997 - val_loss: 0.1364 - val_acc: 0.8175\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1460 - acc: 0.8014 - val_loss: 0.1362 - val_acc: 0.8175\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1461 - acc: 0.7962 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1459 - acc: 0.8031 - val_loss: 0.1363 - val_acc: 0.8175\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1460 - acc: 0.8014 - val_loss: 0.1360 - val_acc: 0.8175\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1458 - acc: 0.8014 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 00119: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.2692 - acc: 0.4503 - val_loss: 0.2555 - val_acc: 0.5159\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.2555 - acc: 0.5034 - val_loss: 0.2441 - val_acc: 0.6111\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.2456 - acc: 0.5908 - val_loss: 0.2340 - val_acc: 0.7698\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.2363 - acc: 0.6815 - val_loss: 0.2245 - val_acc: 0.7857\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.2286 - acc: 0.6969 - val_loss: 0.2158 - val_acc: 0.7857\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.2209 - acc: 0.7534 - val_loss: 0.2082 - val_acc: 0.8016\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.2143 - acc: 0.7637 - val_loss: 0.2006 - val_acc: 0.8175\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.2087 - acc: 0.7723 - val_loss: 0.1937 - val_acc: 0.8095\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.2027 - acc: 0.7705 - val_loss: 0.1874 - val_acc: 0.8095\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.700 - 0s 72us/step - loss: 0.1968 - acc: 0.7740 - val_loss: 0.1824 - val_acc: 0.8254\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1924 - acc: 0.7808 - val_loss: 0.1766 - val_acc: 0.8095\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1885 - acc: 0.7740 - val_loss: 0.1722 - val_acc: 0.8175\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1845 - acc: 0.7808 - val_loss: 0.1678 - val_acc: 0.8175\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1808 - acc: 0.7842 - val_loss: 0.1644 - val_acc: 0.8095\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1785 - acc: 0.7774 - val_loss: 0.1608 - val_acc: 0.8175\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1748 - acc: 0.7825 - val_loss: 0.1579 - val_acc: 0.8175\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1723 - acc: 0.7860 - val_loss: 0.1553 - val_acc: 0.8175\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1710 - acc: 0.7860 - val_loss: 0.1531 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1694 - acc: 0.7860 - val_loss: 0.1509 - val_acc: 0.8175\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.7842 - val_loss: 0.1490 - val_acc: 0.8175\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1648 - acc: 0.7825 - val_loss: 0.1478 - val_acc: 0.8175\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 134us/step - loss: 0.1630 - acc: 0.7928 - val_loss: 0.1459 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1622 - acc: 0.7860 - val_loss: 0.1446 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1600 - acc: 0.7842 - val_loss: 0.1439 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1596 - acc: 0.7911 - val_loss: 0.1424 - val_acc: 0.8175\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1580 - acc: 0.7825 - val_loss: 0.1418 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1576 - acc: 0.7860 - val_loss: 0.1411 - val_acc: 0.8175\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1567 - acc: 0.7825 - val_loss: 0.1401 - val_acc: 0.8175\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1557 - acc: 0.7842 - val_loss: 0.1394 - val_acc: 0.8175\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1554 - acc: 0.7825 - val_loss: 0.1388 - val_acc: 0.8175\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1564 - acc: 0.7877 - val_loss: 0.1387 - val_acc: 0.8175\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1539 - acc: 0.7894 - val_loss: 0.1379 - val_acc: 0.8175\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1535 - acc: 0.7860 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1536 - acc: 0.7894 - val_loss: 0.1378 - val_acc: 0.8175\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1531 - acc: 0.7911 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1520 - acc: 0.7894 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1520 - acc: 0.7911 - val_loss: 0.1363 - val_acc: 0.8175\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1515 - acc: 0.7860 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1516 - acc: 0.7911 - val_loss: 0.1358 - val_acc: 0.8175\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1508 - acc: 0.7877 - val_loss: 0.1356 - val_acc: 0.8175\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1509 - acc: 0.7894 - val_loss: 0.1354 - val_acc: 0.8175\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1503 - acc: 0.7911 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1502 - acc: 0.7945 - val_loss: 0.1355 - val_acc: 0.8175\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1504 - acc: 0.7877 - val_loss: 0.1349 - val_acc: 0.8175\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1496 - acc: 0.7962 - val_loss: 0.1352 - val_acc: 0.8175\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1496 - acc: 0.7894 - val_loss: 0.1346 - val_acc: 0.8175\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 108us/step - loss: 0.1493 - acc: 0.7911 - val_loss: 0.1345 - val_acc: 0.8175\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1492 - acc: 0.7911 - val_loss: 0.1343 - val_acc: 0.8175\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1488 - acc: 0.7928 - val_loss: 0.1341 - val_acc: 0.8175\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1484 - acc: 0.7911 - val_loss: 0.1343 - val_acc: 0.8175\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1481 - acc: 0.7928 - val_loss: 0.1338 - val_acc: 0.8175\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1480 - acc: 0.7928 - val_loss: 0.1339 - val_acc: 0.8175\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1478 - acc: 0.7979 - val_loss: 0.1340 - val_acc: 0.8175\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1477 - acc: 0.7911 - val_loss: 0.1336 - val_acc: 0.8175\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1476 - acc: 0.7945 - val_loss: 0.1335 - val_acc: 0.8175\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1481 - acc: 0.7962 - val_loss: 0.1334 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1474 - acc: 0.8014 - val_loss: 0.1340 - val_acc: 0.8175\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1468 - acc: 0.7979 - val_loss: 0.1332 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1467 - acc: 0.7979 - val_loss: 0.1331 - val_acc: 0.8175\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1471 - acc: 0.7979 - val_loss: 0.1334 - val_acc: 0.8175\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1464 - acc: 0.8048 - val_loss: 0.1329 - val_acc: 0.8175\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1466 - acc: 0.8031 - val_loss: 0.1327 - val_acc: 0.8254\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1468 - acc: 0.7962 - val_loss: 0.1331 - val_acc: 0.8175\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1463 - acc: 0.8082 - val_loss: 0.1333 - val_acc: 0.8175\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1456 - acc: 0.7997 - val_loss: 0.1325 - val_acc: 0.8254\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1457 - acc: 0.7928 - val_loss: 0.1326 - val_acc: 0.8175\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1460 - acc: 0.8065 - val_loss: 0.1326 - val_acc: 0.8175\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1455 - acc: 0.8014 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1455 - acc: 0.8048 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 58us/step - loss: 0.1451 - acc: 0.7962 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1447 - acc: 0.8048 - val_loss: 0.1324 - val_acc: 0.8175\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1446 - acc: 0.8065 - val_loss: 0.1321 - val_acc: 0.8254\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1450 - acc: 0.7979 - val_loss: 0.1323 - val_acc: 0.8175\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1445 - acc: 0.7979 - val_loss: 0.1320 - val_acc: 0.8254\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1450 - acc: 0.8014 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1442 - acc: 0.8082 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1439 - acc: 0.8082 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1438 - acc: 0.8116 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1437 - acc: 0.8082 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1440 - acc: 0.8082 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1436 - acc: 0.8099 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1433 - acc: 0.8082 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1434 - acc: 0.8082 - val_loss: 0.1320 - val_acc: 0.8175\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 69us/step - loss: 0.1436 - acc: 0.8099 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 77us/step - loss: 0.1434 - acc: 0.7894 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1429 - acc: 0.8082 - val_loss: 0.1316 - val_acc: 0.8175\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1428 - acc: 0.8082 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 66us/step - loss: 0.1431 - acc: 0.8014 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.1424 - acc: 0.8099 - val_loss: 0.1319 - val_acc: 0.8175\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1425 - acc: 0.8116 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1425 - acc: 0.8082 - val_loss: 0.1310 - val_acc: 0.8254\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 59us/step - loss: 0.1425 - acc: 0.7945 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.1435 - acc: 0.8116 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.1418 - acc: 0.8099 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1423 - acc: 0.8065 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1422 - acc: 0.7997 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1426 - acc: 0.8099 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1418 - acc: 0.8134 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 68us/step - loss: 0.1420 - acc: 0.8082 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.1423 - acc: 0.7979 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 00100: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.7162 - acc: 0.5154 - val_loss: 0.4364 - val_acc: 0.5317\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.3390 - acc: 0.5154 - val_loss: 0.2532 - val_acc: 0.5317\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.2405 - acc: 0.5462 - val_loss: 0.2282 - val_acc: 0.7460\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.2315 - acc: 0.7483 - val_loss: 0.2263 - val_acc: 0.7857\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.2291 - acc: 0.7551 - val_loss: 0.2231 - val_acc: 0.8016\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.2264 - acc: 0.7517 - val_loss: 0.2202 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 60us/step - loss: 0.2239 - acc: 0.7414 - val_loss: 0.2171 - val_acc: 0.7937\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 64us/step - loss: 0.2215 - acc: 0.7620 - val_loss: 0.2143 - val_acc: 0.8095\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.2186 - acc: 0.7620 - val_loss: 0.2113 - val_acc: 0.8254\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.2162 - acc: 0.7637 - val_loss: 0.2082 - val_acc: 0.8254\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2135 - acc: 0.7637 - val_loss: 0.2054 - val_acc: 0.8254\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.2110 - acc: 0.7757 - val_loss: 0.2024 - val_acc: 0.8175\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.2084 - acc: 0.7774 - val_loss: 0.1995 - val_acc: 0.8254\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 63us/step - loss: 0.2062 - acc: 0.7774 - val_loss: 0.1969 - val_acc: 0.8095\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 62us/step - loss: 0.2039 - acc: 0.7740 - val_loss: 0.1941 - val_acc: 0.8095\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 61us/step - loss: 0.2012 - acc: 0.7825 - val_loss: 0.1914 - val_acc: 0.8175\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1990 - acc: 0.7774 - val_loss: 0.1888 - val_acc: 0.8254\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1965 - acc: 0.7842 - val_loss: 0.1864 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1949 - acc: 0.7774 - val_loss: 0.1841 - val_acc: 0.8175\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1922 - acc: 0.7825 - val_loss: 0.1819 - val_acc: 0.8175\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1905 - acc: 0.7860 - val_loss: 0.1794 - val_acc: 0.8095\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 71us/step - loss: 0.1882 - acc: 0.7877 - val_loss: 0.1773 - val_acc: 0.8095\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1864 - acc: 0.7825 - val_loss: 0.1752 - val_acc: 0.8175\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1847 - acc: 0.7808 - val_loss: 0.1731 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1830 - acc: 0.7894 - val_loss: 0.1714 - val_acc: 0.8095\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1814 - acc: 0.7842 - val_loss: 0.1697 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 72us/step - loss: 0.1797 - acc: 0.7877 - val_loss: 0.1679 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 75us/step - loss: 0.1780 - acc: 0.7894 - val_loss: 0.1660 - val_acc: 0.8095\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 73us/step - loss: 0.1766 - acc: 0.7825 - val_loss: 0.1644 - val_acc: 0.8175\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1749 - acc: 0.7825 - val_loss: 0.1629 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1735 - acc: 0.7860 - val_loss: 0.1613 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1723 - acc: 0.7842 - val_loss: 0.1601 - val_acc: 0.8095\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 69us/step - loss: 0.1713 - acc: 0.7894 - val_loss: 0.1586 - val_acc: 0.8175\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1700 - acc: 0.7860 - val_loss: 0.1575 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1687 - acc: 0.7842 - val_loss: 0.1561 - val_acc: 0.8175\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1676 - acc: 0.7842 - val_loss: 0.1551 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1669 - acc: 0.7825 - val_loss: 0.1542 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.7877 - val_loss: 0.1531 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 108us/step - loss: 0.1650 - acc: 0.7860 - val_loss: 0.1523 - val_acc: 0.8095\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1639 - acc: 0.7877 - val_loss: 0.1511 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1632 - acc: 0.7911 - val_loss: 0.1506 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 67us/step - loss: 0.1620 - acc: 0.7877 - val_loss: 0.1494 - val_acc: 0.8175\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1618 - acc: 0.7842 - val_loss: 0.1486 - val_acc: 0.8095\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 80us/step - loss: 0.1614 - acc: 0.7979 - val_loss: 0.1483 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1611 - acc: 0.7860 - val_loss: 0.1472 - val_acc: 0.8175\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1595 - acc: 0.7928 - val_loss: 0.1471 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1590 - acc: 0.7877 - val_loss: 0.1460 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1582 - acc: 0.7877 - val_loss: 0.1456 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1579 - acc: 0.7877 - val_loss: 0.1449 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1575 - acc: 0.7877 - val_loss: 0.1443 - val_acc: 0.8175\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1570 - acc: 0.7860 - val_loss: 0.1443 - val_acc: 0.8095\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1564 - acc: 0.7894 - val_loss: 0.1435 - val_acc: 0.8095\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1558 - acc: 0.7894 - val_loss: 0.1433 - val_acc: 0.8095\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1558 - acc: 0.7877 - val_loss: 0.1427 - val_acc: 0.8095\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1550 - acc: 0.7962 - val_loss: 0.1429 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1548 - acc: 0.7894 - val_loss: 0.1420 - val_acc: 0.8095\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1542 - acc: 0.7911 - val_loss: 0.1418 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1542 - acc: 0.7928 - val_loss: 0.1419 - val_acc: 0.8095\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1550 - acc: 0.7877 - val_loss: 0.1410 - val_acc: 0.8254\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 78us/step - loss: 0.1549 - acc: 0.7962 - val_loss: 0.1416 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1537 - acc: 0.7877 - val_loss: 0.1406 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1527 - acc: 0.7945 - val_loss: 0.1410 - val_acc: 0.8095\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1525 - acc: 0.7877 - val_loss: 0.1402 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1521 - acc: 0.7911 - val_loss: 0.1400 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1522 - acc: 0.7962 - val_loss: 0.1401 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1517 - acc: 0.7877 - val_loss: 0.1395 - val_acc: 0.8095\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1518 - acc: 0.7997 - val_loss: 0.1399 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1517 - acc: 0.7928 - val_loss: 0.1392 - val_acc: 0.8175\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1516 - acc: 0.7962 - val_loss: 0.1397 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1509 - acc: 0.7928 - val_loss: 0.1388 - val_acc: 0.8254\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1509 - acc: 0.7962 - val_loss: 0.1391 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1504 - acc: 0.7979 - val_loss: 0.1386 - val_acc: 0.8175\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 149us/step - loss: 0.1503 - acc: 0.7945 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1501 - acc: 0.7928 - val_loss: 0.1383 - val_acc: 0.8175\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 147us/step - loss: 0.1500 - acc: 0.7877 - val_loss: 0.1384 - val_acc: 0.8095\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 158us/step - loss: 0.1497 - acc: 0.7962 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1503 - acc: 0.8014 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1501 - acc: 0.8048 - val_loss: 0.1387 - val_acc: 0.8095\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1494 - acc: 0.7911 - val_loss: 0.1378 - val_acc: 0.8254\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1494 - acc: 0.7911 - val_loss: 0.1381 - val_acc: 0.8095\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1490 - acc: 0.7962 - val_loss: 0.1378 - val_acc: 0.8095\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1490 - acc: 0.8014 - val_loss: 0.1379 - val_acc: 0.8095\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 138us/step - loss: 0.1486 - acc: 0.7962 - val_loss: 0.1376 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 134us/step - loss: 0.1485 - acc: 0.7962 - val_loss: 0.1376 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1483 - acc: 0.7962 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1487 - acc: 0.7997 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1485 - acc: 0.7928 - val_loss: 0.1372 - val_acc: 0.8175\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1482 - acc: 0.7911 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1480 - acc: 0.8099 - val_loss: 0.1376 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1477 - acc: 0.7945 - val_loss: 0.1370 - val_acc: 0.8254\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1476 - acc: 0.7962 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1475 - acc: 0.7962 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1475 - acc: 0.8014 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1473 - acc: 0.7962 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1474 - acc: 0.8048 - val_loss: 0.1368 - val_acc: 0.8175\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1471 - acc: 0.7979 - val_loss: 0.1366 - val_acc: 0.8254\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 129us/step - loss: 0.1470 - acc: 0.7962 - val_loss: 0.1364 - val_acc: 0.8254\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1469 - acc: 0.7962 - val_loss: 0.1363 - val_acc: 0.8254\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1471 - acc: 0.7928 - val_loss: 0.1366 - val_acc: 0.8175\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 136us/step - loss: 0.1466 - acc: 0.8065 - val_loss: 0.1373 - val_acc: 0.8095\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1464 - acc: 0.7997 - val_loss: 0.1361 - val_acc: 0.8254\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1467 - acc: 0.7945 - val_loss: 0.1365 - val_acc: 0.8095\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1463 - acc: 0.8031 - val_loss: 0.1367 - val_acc: 0.8095\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 123us/step - loss: 0.1461 - acc: 0.8048 - val_loss: 0.1362 - val_acc: 0.8254\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1462 - acc: 0.8031 - val_loss: 0.1361 - val_acc: 0.8254\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1460 - acc: 0.7962 - val_loss: 0.1359 - val_acc: 0.8254\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1460 - acc: 0.8014 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1465 - acc: 0.7911 - val_loss: 0.1358 - val_acc: 0.8254\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1457 - acc: 0.8048 - val_loss: 0.1363 - val_acc: 0.8095\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1458 - acc: 0.8065 - val_loss: 0.1359 - val_acc: 0.8254\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1455 - acc: 0.7979 - val_loss: 0.1357 - val_acc: 0.8254\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1454 - acc: 0.7997 - val_loss: 0.1357 - val_acc: 0.8254\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1453 - acc: 0.8014 - val_loss: 0.1358 - val_acc: 0.8254\n",
      "Epoch 114/200\n",
      "584/584 [==============================] - 0s 128us/step - loss: 0.1452 - acc: 0.8065 - val_loss: 0.1360 - val_acc: 0.8175\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1452 - acc: 0.8031 - val_loss: 0.1357 - val_acc: 0.8254\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1450 - acc: 0.8031 - val_loss: 0.1356 - val_acc: 0.8254\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1449 - acc: 0.8031 - val_loss: 0.1357 - val_acc: 0.8175\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1451 - acc: 0.8065 - val_loss: 0.1356 - val_acc: 0.8254\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1449 - acc: 0.7997 - val_loss: 0.1352 - val_acc: 0.8254\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 133us/step - loss: 0.1450 - acc: 0.7945 - val_loss: 0.1353 - val_acc: 0.8254\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1450 - acc: 0.7928 - val_loss: 0.1355 - val_acc: 0.8254\n",
      "Epoch 00121: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.2621 - acc: 0.5120 - val_loss: 0.2477 - val_acc: 0.5317\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 108us/step - loss: 0.2453 - acc: 0.5548 - val_loss: 0.2337 - val_acc: 0.8016\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.2338 - acc: 0.6610 - val_loss: 0.2222 - val_acc: 0.8175\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.2241 - acc: 0.7586 - val_loss: 0.2135 - val_acc: 0.8254\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.2150 - acc: 0.7860 - val_loss: 0.2034 - val_acc: 0.8095\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.2072 - acc: 0.7979 - val_loss: 0.1949 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.2008 - acc: 0.7911 - val_loss: 0.1871 - val_acc: 0.8095\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 171us/step - loss: 0.1938 - acc: 0.7860 - val_loss: 0.1813 - val_acc: 0.8175\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 167us/step - loss: 0.1876 - acc: 0.7962 - val_loss: 0.1746 - val_acc: 0.8095\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 141us/step - loss: 0.1826 - acc: 0.7894 - val_loss: 0.1688 - val_acc: 0.8095\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1785 - acc: 0.7808 - val_loss: 0.1639 - val_acc: 0.8095\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1738 - acc: 0.7928 - val_loss: 0.1597 - val_acc: 0.8095\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1703 - acc: 0.7911 - val_loss: 0.1564 - val_acc: 0.8175\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1670 - acc: 0.7997 - val_loss: 0.1530 - val_acc: 0.8095\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1646 - acc: 0.7928 - val_loss: 0.1507 - val_acc: 0.8095\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1628 - acc: 0.7877 - val_loss: 0.1479 - val_acc: 0.8175\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1596 - acc: 0.7979 - val_loss: 0.1458 - val_acc: 0.8095\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1581 - acc: 0.7979 - val_loss: 0.1443 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1563 - acc: 0.7997 - val_loss: 0.1428 - val_acc: 0.8095\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1549 - acc: 0.8031 - val_loss: 0.1415 - val_acc: 0.8095\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 159us/step - loss: 0.1540 - acc: 0.7979 - val_loss: 0.1405 - val_acc: 0.8095\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 162us/step - loss: 0.1527 - acc: 0.8031 - val_loss: 0.1394 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 148us/step - loss: 0.1517 - acc: 0.7928 - val_loss: 0.1390 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 152us/step - loss: 0.1508 - acc: 0.8031 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 153us/step - loss: 0.1504 - acc: 0.7979 - val_loss: 0.1383 - val_acc: 0.8095\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1510 - acc: 0.7962 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1496 - acc: 0.8014 - val_loss: 0.1366 - val_acc: 0.8175\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1495 - acc: 0.7962 - val_loss: 0.1382 - val_acc: 0.8016\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1482 - acc: 0.8048 - val_loss: 0.1363 - val_acc: 0.8175\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1495 - acc: 0.8048 - val_loss: 0.1365 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1478 - acc: 0.7997 - val_loss: 0.1358 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1473 - acc: 0.7945 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1478 - acc: 0.8031 - val_loss: 0.1352 - val_acc: 0.8175\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1472 - acc: 0.7997 - val_loss: 0.1366 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1465 - acc: 0.7997 - val_loss: 0.1347 - val_acc: 0.8175\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1464 - acc: 0.7979 - val_loss: 0.1359 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1456 - acc: 0.8082 - val_loss: 0.1344 - val_acc: 0.8175\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 76us/step - loss: 0.1456 - acc: 0.8065 - val_loss: 0.1346 - val_acc: 0.8175\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1463 - acc: 0.8048 - val_loss: 0.1342 - val_acc: 0.8175\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1458 - acc: 0.8031 - val_loss: 0.1340 - val_acc: 0.8175\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1457 - acc: 0.8048 - val_loss: 0.1339 - val_acc: 0.8175\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1452 - acc: 0.8065 - val_loss: 0.1341 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1448 - acc: 0.8065 - val_loss: 0.1348 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1454 - acc: 0.8048 - val_loss: 0.1348 - val_acc: 0.8095\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1447 - acc: 0.8065 - val_loss: 0.1337 - val_acc: 0.8254\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1444 - acc: 0.7962 - val_loss: 0.1362 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1470 - acc: 0.8082 - val_loss: 0.1337 - val_acc: 0.8254\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1437 - acc: 0.8031 - val_loss: 0.1340 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1432 - acc: 0.8099 - val_loss: 0.1332 - val_acc: 0.8175\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1430 - acc: 0.8065 - val_loss: 0.1337 - val_acc: 0.8175\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1433 - acc: 0.8099 - val_loss: 0.1336 - val_acc: 0.8175\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1434 - acc: 0.8082 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1430 - acc: 0.8134 - val_loss: 0.1332 - val_acc: 0.8175\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1429 - acc: 0.8082 - val_loss: 0.1328 - val_acc: 0.8254\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1427 - acc: 0.8151 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1423 - acc: 0.8099 - val_loss: 0.1327 - val_acc: 0.8254\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1418 - acc: 0.8048 - val_loss: 0.1343 - val_acc: 0.8095\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1431 - acc: 0.8099 - val_loss: 0.1327 - val_acc: 0.8254\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1420 - acc: 0.8048 - val_loss: 0.1328 - val_acc: 0.8175\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1434 - acc: 0.8116 - val_loss: 0.1326 - val_acc: 0.8254\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1426 - acc: 0.8099 - val_loss: 0.1334 - val_acc: 0.8095\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1416 - acc: 0.8134 - val_loss: 0.1324 - val_acc: 0.8254\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1413 - acc: 0.8082 - val_loss: 0.1325 - val_acc: 0.8175\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1421 - acc: 0.7997 - val_loss: 0.1340 - val_acc: 0.8095\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1420 - acc: 0.8116 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1409 - acc: 0.8082 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1421 - acc: 0.8099 - val_loss: 0.1321 - val_acc: 0.8175\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 70us/step - loss: 0.1425 - acc: 0.8099 - val_loss: 0.1320 - val_acc: 0.8254\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1434 - acc: 0.8031 - val_loss: 0.1332 - val_acc: 0.8095\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1417 - acc: 0.8151 - val_loss: 0.1326 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1423 - acc: 0.8185 - val_loss: 0.1328 - val_acc: 0.8095\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1407 - acc: 0.8014 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1411 - acc: 0.8151 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1407 - acc: 0.8082 - val_loss: 0.1323 - val_acc: 0.8175\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1400 - acc: 0.8151 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1399 - acc: 0.8082 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1397 - acc: 0.8185 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1398 - acc: 0.8099 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1395 - acc: 0.8151 - val_loss: 0.1322 - val_acc: 0.8175\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1394 - acc: 0.8219 - val_loss: 0.1316 - val_acc: 0.8175\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1392 - acc: 0.8134 - val_loss: 0.1315 - val_acc: 0.8175\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1395 - acc: 0.8168 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1401 - acc: 0.8134 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1400 - acc: 0.8116 - val_loss: 0.1313 - val_acc: 0.8175\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1395 - acc: 0.8134 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1387 - acc: 0.8168 - val_loss: 0.1313 - val_acc: 0.8175\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1390 - acc: 0.8151 - val_loss: 0.1313 - val_acc: 0.8175\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1388 - acc: 0.8185 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 00088: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.2308 - acc: 0.6421 - val_loss: 0.2156 - val_acc: 0.6746\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.2143 - acc: 0.7637 - val_loss: 0.2029 - val_acc: 0.7937\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.2053 - acc: 0.8099 - val_loss: 0.1933 - val_acc: 0.8254\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1973 - acc: 0.7860 - val_loss: 0.1847 - val_acc: 0.8333\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1927 - acc: 0.7979 - val_loss: 0.1766 - val_acc: 0.8016\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1817 - acc: 0.7945 - val_loss: 0.1696 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1760 - acc: 0.8014 - val_loss: 0.1649 - val_acc: 0.8095\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1737 - acc: 0.7928 - val_loss: 0.1589 - val_acc: 0.8095\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1688 - acc: 0.7979 - val_loss: 0.1547 - val_acc: 0.8095\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1636 - acc: 0.8014 - val_loss: 0.1517 - val_acc: 0.8016\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1618 - acc: 0.7877 - val_loss: 0.1486 - val_acc: 0.8095\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1590 - acc: 0.8048 - val_loss: 0.1459 - val_acc: 0.8095\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1571 - acc: 0.8014 - val_loss: 0.1448 - val_acc: 0.8175\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1559 - acc: 0.7997 - val_loss: 0.1464 - val_acc: 0.8095\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 88us/step - loss: 0.1536 - acc: 0.7911 - val_loss: 0.1408 - val_acc: 0.8095\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1513 - acc: 0.8048 - val_loss: 0.1403 - val_acc: 0.8095\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1511 - acc: 0.7962 - val_loss: 0.1400 - val_acc: 0.8095\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1510 - acc: 0.8048 - val_loss: 0.1386 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1499 - acc: 0.8031 - val_loss: 0.1374 - val_acc: 0.8095\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1481 - acc: 0.8048 - val_loss: 0.1366 - val_acc: 0.8095\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1480 - acc: 0.7979 - val_loss: 0.1380 - val_acc: 0.8095\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1468 - acc: 0.8048 - val_loss: 0.1361 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1463 - acc: 0.7997 - val_loss: 0.1362 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1463 - acc: 0.8048 - val_loss: 0.1358 - val_acc: 0.8095\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1461 - acc: 0.7997 - val_loss: 0.1349 - val_acc: 0.8095\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1453 - acc: 0.8031 - val_loss: 0.1346 - val_acc: 0.8095\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1456 - acc: 0.7945 - val_loss: 0.1349 - val_acc: 0.8095\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1447 - acc: 0.8014 - val_loss: 0.1343 - val_acc: 0.8095\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 79us/step - loss: 0.1445 - acc: 0.8031 - val_loss: 0.1344 - val_acc: 0.8095\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1442 - acc: 0.8082 - val_loss: 0.1344 - val_acc: 0.8095\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1447 - acc: 0.8014 - val_loss: 0.1338 - val_acc: 0.8095\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1445 - acc: 0.7979 - val_loss: 0.1371 - val_acc: 0.8175\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1434 - acc: 0.8031 - val_loss: 0.1339 - val_acc: 0.8095\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1453 - acc: 0.8134 - val_loss: 0.1335 - val_acc: 0.8095\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1432 - acc: 0.8099 - val_loss: 0.1338 - val_acc: 0.8095\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1427 - acc: 0.8065 - val_loss: 0.1336 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1423 - acc: 0.8082 - val_loss: 0.1331 - val_acc: 0.8095\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1425 - acc: 0.8099 - val_loss: 0.1331 - val_acc: 0.8095\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1431 - acc: 0.8082 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1428 - acc: 0.8099 - val_loss: 0.1331 - val_acc: 0.8095\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 74us/step - loss: 0.1421 - acc: 0.8065 - val_loss: 0.1346 - val_acc: 0.8095\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1430 - acc: 0.8082 - val_loss: 0.1328 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1426 - acc: 0.8099 - val_loss: 0.1327 - val_acc: 0.8095\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1423 - acc: 0.7979 - val_loss: 0.1352 - val_acc: 0.8175\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1418 - acc: 0.8082 - val_loss: 0.1327 - val_acc: 0.8175\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1423 - acc: 0.8065 - val_loss: 0.1326 - val_acc: 0.8095\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1409 - acc: 0.8134 - val_loss: 0.1325 - val_acc: 0.8095\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1406 - acc: 0.8116 - val_loss: 0.1323 - val_acc: 0.8095\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1408 - acc: 0.8151 - val_loss: 0.1323 - val_acc: 0.8095\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1407 - acc: 0.8116 - val_loss: 0.1323 - val_acc: 0.8095\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1409 - acc: 0.8116 - val_loss: 0.1327 - val_acc: 0.8175\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1419 - acc: 0.8048 - val_loss: 0.1321 - val_acc: 0.8175\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1403 - acc: 0.8082 - val_loss: 0.1345 - val_acc: 0.8175\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1422 - acc: 0.8168 - val_loss: 0.1319 - val_acc: 0.8175\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1399 - acc: 0.8151 - val_loss: 0.1322 - val_acc: 0.8095\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - ETA: 0s - loss: 0.1394 - acc: 0.807 - 0s 137us/step - loss: 0.1396 - acc: 0.8099 - val_loss: 0.1318 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1402 - acc: 0.8151 - val_loss: 0.1319 - val_acc: 0.8175\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1398 - acc: 0.8082 - val_loss: 0.1319 - val_acc: 0.8016\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1403 - acc: 0.8134 - val_loss: 0.1317 - val_acc: 0.8175\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1395 - acc: 0.8134 - val_loss: 0.1326 - val_acc: 0.8095\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1396 - acc: 0.8185 - val_loss: 0.1316 - val_acc: 0.8175\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1400 - acc: 0.8202 - val_loss: 0.1317 - val_acc: 0.8016\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1398 - acc: 0.8065 - val_loss: 0.1330 - val_acc: 0.8095\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1414 - acc: 0.8151 - val_loss: 0.1323 - val_acc: 0.8254\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1396 - acc: 0.8082 - val_loss: 0.1332 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1384 - acc: 0.8134 - val_loss: 0.1314 - val_acc: 0.8175\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1388 - acc: 0.8134 - val_loss: 0.1324 - val_acc: 0.8095\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 128us/step - loss: 0.1390 - acc: 0.8116 - val_loss: 0.1316 - val_acc: 0.8175\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1388 - acc: 0.8185 - val_loss: 0.1314 - val_acc: 0.8175\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 134us/step - loss: 0.1387 - acc: 0.8168 - val_loss: 0.1312 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1385 - acc: 0.8185 - val_loss: 0.1321 - val_acc: 0.8016\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 128us/step - loss: 0.1384 - acc: 0.8116 - val_loss: 0.1311 - val_acc: 0.8175\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 134us/step - loss: 0.1382 - acc: 0.8151 - val_loss: 0.1314 - val_acc: 0.8095\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1387 - acc: 0.8168 - val_loss: 0.1317 - val_acc: 0.8016\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 117us/step - loss: 0.1380 - acc: 0.8134 - val_loss: 0.1311 - val_acc: 0.8175\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1383 - acc: 0.8168 - val_loss: 0.1313 - val_acc: 0.8095\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1379 - acc: 0.8168 - val_loss: 0.1312 - val_acc: 0.8175\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1379 - acc: 0.8134 - val_loss: 0.1310 - val_acc: 0.8175\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1378 - acc: 0.8151 - val_loss: 0.1309 - val_acc: 0.8175\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1384 - acc: 0.8185 - val_loss: 0.1309 - val_acc: 0.8175\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1390 - acc: 0.8082 - val_loss: 0.1326 - val_acc: 0.8016\n",
      "Epoch 00081: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 2.9583 - acc: 0.0000e+00 - val_loss: 1.2952 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.7027 - acc: 0.4675 - val_loss: 0.2897 - val_acc: 0.5317\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.2306 - acc: 0.6387 - val_loss: 0.2138 - val_acc: 0.8095\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 135us/step - loss: 0.2191 - acc: 0.7226 - val_loss: 0.2142 - val_acc: 0.7857\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.2147 - acc: 0.7911 - val_loss: 0.2084 - val_acc: 0.8254\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.2118 - acc: 0.8151 - val_loss: 0.2059 - val_acc: 0.8254\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.2096 - acc: 0.8151 - val_loss: 0.2034 - val_acc: 0.8254\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.2073 - acc: 0.8168 - val_loss: 0.2009 - val_acc: 0.8254\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.2051 - acc: 0.8151 - val_loss: 0.1984 - val_acc: 0.8175\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 136us/step - loss: 0.2030 - acc: 0.8099 - val_loss: 0.1960 - val_acc: 0.8254\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.2010 - acc: 0.8099 - val_loss: 0.1932 - val_acc: 0.8175\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1988 - acc: 0.8134 - val_loss: 0.1906 - val_acc: 0.8175\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1964 - acc: 0.8065 - val_loss: 0.1884 - val_acc: 0.8254\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1943 - acc: 0.8014 - val_loss: 0.1858 - val_acc: 0.8254\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 135us/step - loss: 0.1920 - acc: 0.8116 - val_loss: 0.1831 - val_acc: 0.8254\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1897 - acc: 0.8082 - val_loss: 0.1808 - val_acc: 0.8175\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1877 - acc: 0.8099 - val_loss: 0.1787 - val_acc: 0.8254\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 132us/step - loss: 0.1862 - acc: 0.8048 - val_loss: 0.1760 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1836 - acc: 0.8099 - val_loss: 0.1738 - val_acc: 0.8254\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1817 - acc: 0.8082 - val_loss: 0.1717 - val_acc: 0.8175\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1801 - acc: 0.7997 - val_loss: 0.1696 - val_acc: 0.8175\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1781 - acc: 0.8048 - val_loss: 0.1675 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1766 - acc: 0.8031 - val_loss: 0.1656 - val_acc: 0.8175\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1746 - acc: 0.8014 - val_loss: 0.1637 - val_acc: 0.8175\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1729 - acc: 0.8048 - val_loss: 0.1618 - val_acc: 0.8175\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1713 - acc: 0.8116 - val_loss: 0.1602 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1703 - acc: 0.8014 - val_loss: 0.1586 - val_acc: 0.8175\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 80us/step - loss: 0.1691 - acc: 0.8014 - val_loss: 0.1571 - val_acc: 0.8175\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 139us/step - loss: 0.1676 - acc: 0.8031 - val_loss: 0.1557 - val_acc: 0.8254\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1656 - acc: 0.8048 - val_loss: 0.1542 - val_acc: 0.8175\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1649 - acc: 0.7997 - val_loss: 0.1528 - val_acc: 0.8175\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1634 - acc: 0.8031 - val_loss: 0.1515 - val_acc: 0.8254\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1629 - acc: 0.8048 - val_loss: 0.1503 - val_acc: 0.8175\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1611 - acc: 0.8116 - val_loss: 0.1490 - val_acc: 0.8175\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1603 - acc: 0.8014 - val_loss: 0.1481 - val_acc: 0.8175\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1598 - acc: 0.8048 - val_loss: 0.1472 - val_acc: 0.8175\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1584 - acc: 0.8031 - val_loss: 0.1460 - val_acc: 0.8175\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1579 - acc: 0.8065 - val_loss: 0.1452 - val_acc: 0.8175\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1563 - acc: 0.8099 - val_loss: 0.1443 - val_acc: 0.8254\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1562 - acc: 0.8048 - val_loss: 0.1435 - val_acc: 0.8254\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1552 - acc: 0.8065 - val_loss: 0.1426 - val_acc: 0.8175\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1544 - acc: 0.8082 - val_loss: 0.1419 - val_acc: 0.8175\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 134us/step - loss: 0.1536 - acc: 0.8031 - val_loss: 0.1414 - val_acc: 0.8175\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1531 - acc: 0.8099 - val_loss: 0.1407 - val_acc: 0.8175\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 132us/step - loss: 0.1529 - acc: 0.7997 - val_loss: 0.1399 - val_acc: 0.8175\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1520 - acc: 0.8116 - val_loss: 0.1397 - val_acc: 0.8175\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 127us/step - loss: 0.1515 - acc: 0.8048 - val_loss: 0.1389 - val_acc: 0.8175\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1511 - acc: 0.8048 - val_loss: 0.1384 - val_acc: 0.8175\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1507 - acc: 0.8099 - val_loss: 0.1379 - val_acc: 0.8175\n",
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1501 - acc: 0.7962 - val_loss: 0.1376 - val_acc: 0.8254\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1496 - acc: 0.8031 - val_loss: 0.1373 - val_acc: 0.8175\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1494 - acc: 0.8099 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 104us/step - loss: 0.1491 - acc: 0.8099 - val_loss: 0.1364 - val_acc: 0.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1485 - acc: 0.8048 - val_loss: 0.1362 - val_acc: 0.8175\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1484 - acc: 0.8014 - val_loss: 0.1358 - val_acc: 0.8175\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1491 - acc: 0.8065 - val_loss: 0.1356 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1478 - acc: 0.8082 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1479 - acc: 0.8134 - val_loss: 0.1350 - val_acc: 0.8175\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 133us/step - loss: 0.1472 - acc: 0.8082 - val_loss: 0.1350 - val_acc: 0.8175\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1474 - acc: 0.7997 - val_loss: 0.1351 - val_acc: 0.8175\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1473 - acc: 0.8014 - val_loss: 0.1344 - val_acc: 0.8175\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1464 - acc: 0.7997 - val_loss: 0.1342 - val_acc: 0.8254\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1468 - acc: 0.8099 - val_loss: 0.1341 - val_acc: 0.8175\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 99us/step - loss: 0.1461 - acc: 0.8014 - val_loss: 0.1339 - val_acc: 0.8254\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 112us/step - loss: 0.1459 - acc: 0.8082 - val_loss: 0.1338 - val_acc: 0.8175\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1457 - acc: 0.8116 - val_loss: 0.1335 - val_acc: 0.8254\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1456 - acc: 0.8031 - val_loss: 0.1335 - val_acc: 0.8175\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1456 - acc: 0.8031 - val_loss: 0.1333 - val_acc: 0.8175\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1453 - acc: 0.8065 - val_loss: 0.1333 - val_acc: 0.8175\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1452 - acc: 0.8151 - val_loss: 0.1331 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 132us/step - loss: 0.1446 - acc: 0.8082 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1447 - acc: 0.8014 - val_loss: 0.1330 - val_acc: 0.8175\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1449 - acc: 0.8099 - val_loss: 0.1329 - val_acc: 0.8175\n",
      "Epoch 74/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1442 - acc: 0.8065 - val_loss: 0.1327 - val_acc: 0.8254\n",
      "Epoch 75/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1442 - acc: 0.8116 - val_loss: 0.1326 - val_acc: 0.8175\n",
      "Epoch 76/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1441 - acc: 0.8065 - val_loss: 0.1325 - val_acc: 0.8254\n",
      "Epoch 77/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1436 - acc: 0.8134 - val_loss: 0.1327 - val_acc: 0.8175\n",
      "Epoch 78/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1438 - acc: 0.8134 - val_loss: 0.1325 - val_acc: 0.8175\n",
      "Epoch 79/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1434 - acc: 0.8116 - val_loss: 0.1323 - val_acc: 0.8254\n",
      "Epoch 80/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1434 - acc: 0.8099 - val_loss: 0.1321 - val_acc: 0.8254\n",
      "Epoch 81/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1433 - acc: 0.8031 - val_loss: 0.1324 - val_acc: 0.8175\n",
      "Epoch 82/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1435 - acc: 0.8099 - val_loss: 0.1325 - val_acc: 0.8175\n",
      "Epoch 83/200\n",
      "584/584 [==============================] - 0s 85us/step - loss: 0.1432 - acc: 0.8134 - val_loss: 0.1321 - val_acc: 0.8175\n",
      "Epoch 84/200\n",
      "584/584 [==============================] - 0s 139us/step - loss: 0.1431 - acc: 0.8134 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 85/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1427 - acc: 0.8116 - val_loss: 0.1319 - val_acc: 0.8254\n",
      "Epoch 86/200\n",
      "584/584 [==============================] - 0s 131us/step - loss: 0.1433 - acc: 0.8099 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 87/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1428 - acc: 0.8134 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 88/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1438 - acc: 0.7997 - val_loss: 0.1319 - val_acc: 0.8175\n",
      "Epoch 89/200\n",
      "584/584 [==============================] - 0s 130us/step - loss: 0.1430 - acc: 0.8116 - val_loss: 0.1320 - val_acc: 0.8175\n",
      "Epoch 90/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1429 - acc: 0.8082 - val_loss: 0.1318 - val_acc: 0.8175\n",
      "Epoch 91/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1422 - acc: 0.8134 - val_loss: 0.1317 - val_acc: 0.8175\n",
      "Epoch 92/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1422 - acc: 0.8048 - val_loss: 0.1317 - val_acc: 0.8175\n",
      "Epoch 93/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1421 - acc: 0.8099 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 94/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1421 - acc: 0.8116 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 95/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1418 - acc: 0.7997 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 96/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1419 - acc: 0.8168 - val_loss: 0.1315 - val_acc: 0.8254\n",
      "Epoch 97/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1417 - acc: 0.8065 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 98/200\n",
      "584/584 [==============================] - 0s 137us/step - loss: 0.1418 - acc: 0.8116 - val_loss: 0.1312 - val_acc: 0.8254\n",
      "Epoch 99/200\n",
      "584/584 [==============================] - 0s 90us/step - loss: 0.1414 - acc: 0.8082 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 100/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1417 - acc: 0.8065 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 101/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1417 - acc: 0.8082 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 102/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1413 - acc: 0.8082 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 103/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1417 - acc: 0.8168 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 104/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1410 - acc: 0.8048 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 105/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1411 - acc: 0.8134 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 106/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1407 - acc: 0.8065 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 107/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1412 - acc: 0.8151 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 108/200\n",
      "584/584 [==============================] - 0s 84us/step - loss: 0.1408 - acc: 0.8099 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 109/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1417 - acc: 0.8031 - val_loss: 0.1309 - val_acc: 0.8254\n",
      "Epoch 110/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1412 - acc: 0.8202 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 111/200\n",
      "584/584 [==============================] - 0s 87us/step - loss: 0.1408 - acc: 0.8048 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 112/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1406 - acc: 0.8219 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 113/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1406 - acc: 0.8099 - val_loss: 0.1306 - val_acc: 0.8254\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584/584 [==============================] - 0s 108us/step - loss: 0.1407 - acc: 0.8031 - val_loss: 0.1305 - val_acc: 0.8254\n",
      "Epoch 115/200\n",
      "584/584 [==============================] - 0s 91us/step - loss: 0.1409 - acc: 0.8185 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 116/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1410 - acc: 0.7997 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 117/200\n",
      "584/584 [==============================] - 0s 110us/step - loss: 0.1399 - acc: 0.8185 - val_loss: 0.1304 - val_acc: 0.8254\n",
      "Epoch 118/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1404 - acc: 0.8099 - val_loss: 0.1306 - val_acc: 0.8254\n",
      "Epoch 119/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1396 - acc: 0.8134 - val_loss: 0.1304 - val_acc: 0.8254\n",
      "Epoch 120/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1396 - acc: 0.8202 - val_loss: 0.1306 - val_acc: 0.8254\n",
      "Epoch 121/200\n",
      "584/584 [==============================] - 0s 128us/step - loss: 0.1405 - acc: 0.8065 - val_loss: 0.1306 - val_acc: 0.8254\n",
      "Epoch 122/200\n",
      "584/584 [==============================] - 0s 98us/step - loss: 0.1399 - acc: 0.8134 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 123/200\n",
      "584/584 [==============================] - 0s 82us/step - loss: 0.1398 - acc: 0.8134 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 124/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1391 - acc: 0.8151 - val_loss: 0.1302 - val_acc: 0.8254\n",
      "Epoch 00124: early stopping\n",
      "Train on 584 samples, validate on 126 samples\n",
      "Epoch 1/200\n",
      "584/584 [==============================] - 1s 1ms/step - loss: 0.2616 - acc: 0.5325 - val_loss: 0.2343 - val_acc: 0.4683\n",
      "Epoch 2/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.2219 - acc: 0.6404 - val_loss: 0.2152 - val_acc: 0.5714\n",
      "Epoch 3/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.2152 - acc: 0.7346 - val_loss: 0.2025 - val_acc: 0.8254\n",
      "Epoch 4/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.2055 - acc: 0.7860 - val_loss: 0.1925 - val_acc: 0.8095\n",
      "Epoch 5/200\n",
      "584/584 [==============================] - 0s 86us/step - loss: 0.1995 - acc: 0.7808 - val_loss: 0.1839 - val_acc: 0.8095\n",
      "Epoch 6/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1935 - acc: 0.7825 - val_loss: 0.1777 - val_acc: 0.8095\n",
      "Epoch 7/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1842 - acc: 0.7842 - val_loss: 0.1726 - val_acc: 0.8333\n",
      "Epoch 8/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1793 - acc: 0.7894 - val_loss: 0.1642 - val_acc: 0.8175\n",
      "Epoch 9/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1744 - acc: 0.7842 - val_loss: 0.1593 - val_acc: 0.8254\n",
      "Epoch 10/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1706 - acc: 0.7962 - val_loss: 0.1562 - val_acc: 0.8254\n",
      "Epoch 11/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1686 - acc: 0.7928 - val_loss: 0.1512 - val_acc: 0.8175\n",
      "Epoch 12/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1637 - acc: 0.7894 - val_loss: 0.1500 - val_acc: 0.8175\n",
      "Epoch 13/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1633 - acc: 0.7842 - val_loss: 0.1475 - val_acc: 0.8175\n",
      "Epoch 14/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1590 - acc: 0.7997 - val_loss: 0.1436 - val_acc: 0.8254\n",
      "Epoch 15/200\n",
      "584/584 [==============================] - 0s 117us/step - loss: 0.1569 - acc: 0.7894 - val_loss: 0.1419 - val_acc: 0.8254\n",
      "Epoch 16/200\n",
      "584/584 [==============================] - 0s 139us/step - loss: 0.1580 - acc: 0.7894 - val_loss: 0.1401 - val_acc: 0.8175\n",
      "Epoch 17/200\n",
      "584/584 [==============================] - 0s 92us/step - loss: 0.1561 - acc: 0.7860 - val_loss: 0.1416 - val_acc: 0.8175\n",
      "Epoch 18/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1540 - acc: 0.7928 - val_loss: 0.1380 - val_acc: 0.8175\n",
      "Epoch 19/200\n",
      "584/584 [==============================] - 0s 123us/step - loss: 0.1531 - acc: 0.8014 - val_loss: 0.1372 - val_acc: 0.8175\n",
      "Epoch 20/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1518 - acc: 0.8014 - val_loss: 0.1383 - val_acc: 0.8254\n",
      "Epoch 21/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1502 - acc: 0.7962 - val_loss: 0.1365 - val_acc: 0.8175\n",
      "Epoch 22/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1504 - acc: 0.7911 - val_loss: 0.1377 - val_acc: 0.8175\n",
      "Epoch 23/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1494 - acc: 0.7997 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 24/200\n",
      "584/584 [==============================] - 0s 118us/step - loss: 0.1484 - acc: 0.7997 - val_loss: 0.1357 - val_acc: 0.8095\n",
      "Epoch 25/200\n",
      "584/584 [==============================] - 0s 111us/step - loss: 0.1484 - acc: 0.7997 - val_loss: 0.1353 - val_acc: 0.8175\n",
      "Epoch 26/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1482 - acc: 0.8065 - val_loss: 0.1339 - val_acc: 0.8254\n",
      "Epoch 27/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1477 - acc: 0.8014 - val_loss: 0.1337 - val_acc: 0.8254\n",
      "Epoch 28/200\n",
      "584/584 [==============================] - 0s 109us/step - loss: 0.1477 - acc: 0.7997 - val_loss: 0.1334 - val_acc: 0.8254\n",
      "Epoch 29/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1463 - acc: 0.7997 - val_loss: 0.1333 - val_acc: 0.8254\n",
      "Epoch 30/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1472 - acc: 0.7997 - val_loss: 0.1335 - val_acc: 0.8254\n",
      "Epoch 31/200\n",
      "584/584 [==============================] - 0s 101us/step - loss: 0.1456 - acc: 0.8014 - val_loss: 0.1329 - val_acc: 0.8254\n",
      "Epoch 32/200\n",
      "584/584 [==============================] - 0s 81us/step - loss: 0.1455 - acc: 0.7997 - val_loss: 0.1354 - val_acc: 0.8333\n",
      "Epoch 33/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1488 - acc: 0.7997 - val_loss: 0.1344 - val_acc: 0.8175\n",
      "Epoch 34/200\n",
      "584/584 [==============================] - 0s 116us/step - loss: 0.1480 - acc: 0.8031 - val_loss: 0.1348 - val_acc: 0.8175\n",
      "Epoch 35/200\n",
      "584/584 [==============================] - 0s 120us/step - loss: 0.1448 - acc: 0.8116 - val_loss: 0.1325 - val_acc: 0.8254\n",
      "Epoch 36/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1445 - acc: 0.7997 - val_loss: 0.1370 - val_acc: 0.8095\n",
      "Epoch 37/200\n",
      "584/584 [==============================] - 0s 133us/step - loss: 0.1455 - acc: 0.8116 - val_loss: 0.1324 - val_acc: 0.8254\n",
      "Epoch 38/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1443 - acc: 0.8099 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 39/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1442 - acc: 0.8099 - val_loss: 0.1343 - val_acc: 0.8175\n",
      "Epoch 40/200\n",
      "584/584 [==============================] - 0s 95us/step - loss: 0.1427 - acc: 0.8151 - val_loss: 0.1324 - val_acc: 0.8254\n",
      "Epoch 41/200\n",
      "584/584 [==============================] - 0s 132us/step - loss: 0.1444 - acc: 0.8031 - val_loss: 0.1317 - val_acc: 0.8254\n",
      "Epoch 42/200\n",
      "584/584 [==============================] - 0s 103us/step - loss: 0.1422 - acc: 0.8031 - val_loss: 0.1369 - val_acc: 0.8095\n",
      "Epoch 43/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1467 - acc: 0.8082 - val_loss: 0.1344 - val_acc: 0.8175\n",
      "Epoch 44/200\n",
      "584/584 [==============================] - 0s 96us/step - loss: 0.1451 - acc: 0.8065 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 45/200\n",
      "584/584 [==============================] - 0s 114us/step - loss: 0.1420 - acc: 0.8134 - val_loss: 0.1314 - val_acc: 0.8254\n",
      "Epoch 46/200\n",
      "584/584 [==============================] - 0s 88us/step - loss: 0.1430 - acc: 0.7945 - val_loss: 0.1318 - val_acc: 0.8254\n",
      "Epoch 47/200\n",
      "584/584 [==============================] - 0s 83us/step - loss: 0.1421 - acc: 0.8031 - val_loss: 0.1329 - val_acc: 0.8254\n",
      "Epoch 48/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1418 - acc: 0.8065 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 49/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1425 - acc: 0.8082 - val_loss: 0.1310 - val_acc: 0.8254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1432 - acc: 0.8014 - val_loss: 0.1310 - val_acc: 0.8254\n",
      "Epoch 51/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1422 - acc: 0.8014 - val_loss: 0.1328 - val_acc: 0.8254\n",
      "Epoch 52/200\n",
      "584/584 [==============================] - 0s 105us/step - loss: 0.1406 - acc: 0.8168 - val_loss: 0.1308 - val_acc: 0.8254\n",
      "Epoch 53/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1414 - acc: 0.8048 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 54/200\n",
      "584/584 [==============================] - 0s 113us/step - loss: 0.1425 - acc: 0.8082 - val_loss: 0.1307 - val_acc: 0.8254\n",
      "Epoch 55/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1417 - acc: 0.8099 - val_loss: 0.1313 - val_acc: 0.8254\n",
      "Epoch 56/200\n",
      "584/584 [==============================] - 0s 89us/step - loss: 0.1400 - acc: 0.8099 - val_loss: 0.1313 - val_acc: 0.8175\n",
      "Epoch 57/200\n",
      "584/584 [==============================] - 0s 94us/step - loss: 0.1405 - acc: 0.8082 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 58/200\n",
      "584/584 [==============================] - 0s 93us/step - loss: 0.1397 - acc: 0.8151 - val_loss: 0.1304 - val_acc: 0.8254\n",
      "Epoch 59/200\n",
      "584/584 [==============================] - 0s 107us/step - loss: 0.1397 - acc: 0.8185 - val_loss: 0.1304 - val_acc: 0.8254\n",
      "Epoch 60/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1397 - acc: 0.8065 - val_loss: 0.1316 - val_acc: 0.8254\n",
      "Epoch 61/200\n",
      "584/584 [==============================] - 0s 124us/step - loss: 0.1394 - acc: 0.8134 - val_loss: 0.1311 - val_acc: 0.8254\n",
      "Epoch 62/200\n",
      "584/584 [==============================] - 0s 97us/step - loss: 0.1413 - acc: 0.8065 - val_loss: 0.1310 - val_acc: 0.8254\n",
      "Epoch 63/200\n",
      "584/584 [==============================] - 0s 106us/step - loss: 0.1393 - acc: 0.8151 - val_loss: 0.1301 - val_acc: 0.8254\n",
      "Epoch 64/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1398 - acc: 0.8151 - val_loss: 0.1302 - val_acc: 0.8254\n",
      "Epoch 65/200\n",
      "584/584 [==============================] - 0s 100us/step - loss: 0.1390 - acc: 0.8185 - val_loss: 0.1300 - val_acc: 0.8175\n",
      "Epoch 66/200\n",
      "584/584 [==============================] - 0s 121us/step - loss: 0.1388 - acc: 0.8134 - val_loss: 0.1306 - val_acc: 0.8254\n",
      "Epoch 67/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1396 - acc: 0.8116 - val_loss: 0.1301 - val_acc: 0.8175\n",
      "Epoch 68/200\n",
      "584/584 [==============================] - 0s 126us/step - loss: 0.1390 - acc: 0.8116 - val_loss: 0.1303 - val_acc: 0.8254\n",
      "Epoch 69/200\n",
      "584/584 [==============================] - 0s 122us/step - loss: 0.1385 - acc: 0.8202 - val_loss: 0.1299 - val_acc: 0.8175\n",
      "Epoch 70/200\n",
      "584/584 [==============================] - 0s 119us/step - loss: 0.1387 - acc: 0.8185 - val_loss: 0.1298 - val_acc: 0.8175\n",
      "Epoch 71/200\n",
      "584/584 [==============================] - 0s 115us/step - loss: 0.1382 - acc: 0.8134 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 72/200\n",
      "584/584 [==============================] - 0s 125us/step - loss: 0.1390 - acc: 0.8082 - val_loss: 0.1298 - val_acc: 0.8175\n",
      "Epoch 73/200\n",
      "584/584 [==============================] - 0s 102us/step - loss: 0.1393 - acc: 0.8116 - val_loss: 0.1322 - val_acc: 0.8254\n",
      "Epoch 00073: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Testing different number of neurons\n",
    "neurons = [5,10,15,30]\n",
    "repetitions = 2\n",
    "result = list()\n",
    "for neuron in neurons:\n",
    "    result_i = list()\n",
    "    for i in range(0,repetitions):\n",
    "        lr = 0.001  # learning rate\n",
    "        lr_decay = 0.0005# learning rate decay\n",
    "        n_mini_batch = 100  # mini-batch length\n",
    "        activation_fcn = \"sigmoid\"\n",
    "        optimizer = Adam(lr=lr, decay=lr_decay)\n",
    "        input_dim = x.shape[1]\n",
    "        h_n = neuron\n",
    "        model = Sequential()\n",
    "        model.add(Dense(h_n, input_dim=input_dim, activation=activation_fcn))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train and validate the model\n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            y_train[:, 0],\n",
    "            validation_data=(x_val, y_val[:, 0]),\n",
    "            epochs=200,\n",
    "            batch_size=30,\n",
    "            # verbose=0,\n",
    "            callbacks=[\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", mode=\"min\", min_delta=0.001, patience=20, verbose=1\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        pred = model.predict(x_test)\n",
    "        pred = np.array(pred).flatten()\n",
    "\n",
    "        erro = pred - np.array(y_test[:, 0]).flatten()\n",
    "        erro = np.abs(erro)\n",
    "\n",
    "        acerto = 0\n",
    "        for i in erro:\n",
    "            if i < 0.5:\n",
    "                acerto += 1\n",
    "\n",
    "        result_i.append(acerto)\n",
    "    result.append(result_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array(result).reshape((len(neurons),repetitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [np.mean(i) for i in result]\n",
    "acc  = np.array(result) * (100/len(y_val)) \n",
    "mean_acc = np.array(mean) * (100/len(y_val))\n",
    "\n",
    "result_dict = {'neurons': neurons, 'results_1': result[:,0],'results_2':result[:,1], 'mean': mean, 'acc_1' : acc[:,0],'acc_2' : acc[:,1] , 'mean_acc': mean_acc}\n",
    "result_df = pd.DataFrame(data=result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neurons</th>\n",
       "      <th>results_1</th>\n",
       "      <th>results_2</th>\n",
       "      <th>mean</th>\n",
       "      <th>acc_1</th>\n",
       "      <th>acc_2</th>\n",
       "      <th>mean_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>102</td>\n",
       "      <td>101</td>\n",
       "      <td>101.5</td>\n",
       "      <td>80.952381</td>\n",
       "      <td>80.158730</td>\n",
       "      <td>80.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "      <td>100.5</td>\n",
       "      <td>80.158730</td>\n",
       "      <td>79.365079</td>\n",
       "      <td>79.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>101.5</td>\n",
       "      <td>80.158730</td>\n",
       "      <td>80.952381</td>\n",
       "      <td>80.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>104</td>\n",
       "      <td>102</td>\n",
       "      <td>103.0</td>\n",
       "      <td>82.539683</td>\n",
       "      <td>80.952381</td>\n",
       "      <td>81.746032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>103</td>\n",
       "      <td>100</td>\n",
       "      <td>101.5</td>\n",
       "      <td>81.746032</td>\n",
       "      <td>79.365079</td>\n",
       "      <td>80.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>105</td>\n",
       "      <td>101</td>\n",
       "      <td>103.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>80.158730</td>\n",
       "      <td>81.746032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neurons  results_1  results_2   mean      acc_1      acc_2   mean_acc\n",
       "0        5        102        101  101.5  80.952381  80.158730  80.555556\n",
       "1       10        101        100  100.5  80.158730  79.365079  79.761905\n",
       "2       15        101        102  101.5  80.158730  80.952381  80.555556\n",
       "3       30        104        102  103.0  82.539683  80.952381  81.746032\n",
       "4       60        103        100  101.5  81.746032  79.365079  80.555556\n",
       "5       90        105        101  103.0  83.333333  80.158730  81.746032"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  7]\n",
      " [12 50]]\n"
     ]
    }
   ],
   "source": [
    "valores_reais = np.array(y_test[:,0],dtype=int).flatten()\n",
    "valores_preditos = np.array(np.round(pred),dtype=int).flatten()\n",
    "matrix = get_confusion_matrix(reais=valores_reais, preditos=valores_preditos, labels=[1,0])\n",
    "matrix_df = {'True_positive': matrix[0]}\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
