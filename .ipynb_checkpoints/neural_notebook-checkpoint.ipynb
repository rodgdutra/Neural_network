{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#matplotlib.style.use('classic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Back_prop_mlp():\n",
    "\tdef __init__(self,data_csv='wine2.csv'):\n",
    "\t\tself.load_data(data_csv)\n",
    "\t\tself.norm_dataset(0,13)\n",
    "\t\tself.baseline_model()\n",
    "\n",
    "\tdef norm_dataset(self,start,end):\n",
    "\t\tdataset = list()\n",
    "\n",
    "\t\tfor i in range(start,end):\n",
    "\t\t\tnorm_col = self.dataset[:,i]/np.amax(self.dataset[:,i])\n",
    "\t\t\tdataset.append(norm_col)\n",
    "\n",
    "\t\tfor i in range(end,end+2):\n",
    "\t\t\tcol = self.dataset[:,i]\n",
    "\t\t\tdataset.append(col)\n",
    "\n",
    "\t\tdataset = pd.DataFrame(dataset)\n",
    "\t\tdataset = dataset.transpose()\n",
    "\t\tself.dataset =dataset.values\n",
    "\n",
    "\tdef load_data(self,data_csv):\n",
    "\t\t# load dataset\n",
    "\t\tdataframe = pd.read_csv(data_csv, header=None)\n",
    "\t\tself.dataset = dataframe.values\n",
    "\t\tself.X = self.dataset[:,0:13].astype(float)\n",
    "\t\tself.Y = self.dataset[:,13:15]\n",
    "\n",
    "\tdef baseline_model(self,h_layers=50):\n",
    "\t\t# create model\n",
    "\t\tself.model = Sequential()\n",
    "\t\tself.model.add(Dense(h_layers, input_dim=13,\n",
    "\t\t\t\t\t\t\t activation='sigmoid'))\n",
    "\t\tself.model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\t\t# Compile model\n",
    "\t\tself.model.compile(loss='mse',\n",
    "\t\t\t\t\t\t   optimizer='ADAM',\n",
    "\t\t\t\t\t\t   metrics=['accuracy'])\n",
    "\n",
    "\tdef split_data(self,seed=9,random=True):\n",
    "\t\tif (random):\n",
    "\t\t\t# Split data set into train and validation\n",
    "\t\t\tself.X_train, self.X_val,self.Y_train, self.Y_val = train_test_split(self.X,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.Y,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=seed)\n",
    "\t\t\t# Split train into train and test\n",
    "\t\t\tself.X_train, self.X_test,self.Y_train, self.Y_test = train_test_split(self.X_train,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.Y_train,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.25,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=seed)\n",
    "\t\t\tprint(self.X_train.shape)\n",
    "\n",
    "\tdef train_and_val(self):\n",
    "\t\t# Train and validate the model\n",
    "\t\tself.model.fit(self.X_train,\n",
    "\t\t\t\t\t   self.Y_train[:,0],\n",
    "\t\t\t\t\t   validation_data=(self.X_val,self.Y_val[:,0]),\n",
    "\t\t\t\t\t   epochs=400, batch_size=10,verbose=1)\n",
    "\tdef test(self):\n",
    "\t\tself.net_test = np.round([i[0] for i in self.model.predict(self.X_test)])\n",
    "\n",
    "\tdef total_output(self):\n",
    "\t\tself.net_out = np.round([i[0] for i in self.model.predict(self.X)])\n",
    "\n",
    "\tdef score(self):\n",
    "\t\tscores = self.model.evaluate(self.X_test, self.Y_test[:,0])\n",
    "\t\t# evaluate the model\n",
    "\t\tprint(\"\\n%s: %.2f%%\" % (self.model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "class Auto_associative_mlp():\n",
    "\tdef __init__(self,data_csv='wine2.csv'):\n",
    "\t\tself.load_data(data_csv)\n",
    "\t\tself.norm_dataset(0,13)\n",
    "\t\tself.X = self.dataset[:,0:13].astype(float)\n",
    "\t\tself.Y = self.dataset[:,13:15]\n",
    "\t\tself.X1,self.Y1 = self.split_class_dataset(0,59)\n",
    "\t\tself.X2,self.Y2 = self.split_class_dataset(59,130)\n",
    "\t\tself.X3,self.Y3 = self.split_class_dataset(130,178)\n",
    "\n",
    "\t\tself.model1 = self.baseline_model()\n",
    "\t\tself.model2 = self.baseline_model()\n",
    "\t\tself.model3 = self.baseline_model()\n",
    "\n",
    "\t\tself.split_dataset('1')\n",
    "\t\tself.split_dataset('2')\n",
    "\t\tself.split_dataset('3')\n",
    "\n",
    "\n",
    "\tdef load_data(self,data_csv):\n",
    "\t\t# load dataset\n",
    "\t\tdataframe = pd.read_csv(data_csv, header=None)\n",
    "\t\tself.dataset = dataframe.values\n",
    "\t\t#pprint(dataframe)\n",
    "\n",
    "\tdef split_class_dataset(self,start,end):\n",
    "\t\tX_i = self.dataset[start:end][:,0:13].astype(float)\n",
    "\t\tY_i = self.dataset[start:end][:,13:15]\n",
    "\t\treturn X_i,Y_i\n",
    "\n",
    "\tdef baseline_model(self,h_layers=10):\n",
    "\t\t# create model\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(h_layers, input_dim=13, activation='sigmoid'))\n",
    "\t\tmodel.add(Dense(13, activation='linear'))\n",
    "\t\trms_prop = keras.optimizers.RMSprop(lr=1, rho=0.9, epsilon=None, decay=0)\n",
    "\t\tadam = keras.optimizers.Adam(lr=0.045, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\t\t# Compile model\n",
    "\t\tmodel.compile(loss='mean_squared_error', optimizer=adam,metrics=['acc'])\n",
    "\t\treturn model\n",
    "\n",
    "\tdef norm_dataset(self,start,end):\n",
    "\t\tdataset = list()\n",
    "\n",
    "\t\tfor i in range(start,end):\n",
    "\t\t\tnorm_col = self.dataset[:,i]/np.amax(self.dataset[:,i])\n",
    "\t\t\tdataset.append(norm_col)\n",
    "\n",
    "\t\tfor i in range(end,end+2):\n",
    "\t\t\tcol = self.dataset[:,i]\n",
    "\t\t\tdataset.append(col)\n",
    "\n",
    "\t\tdataset = pd.DataFrame(dataset)\n",
    "\t\tdataset = dataset.transpose()\n",
    "\t\tself.dataset =dataset.values\n",
    "\n",
    "\tdef split_dataset(self,id_i,seed=9):\n",
    "\n",
    "\t\tX = getattr(self,'X'+id_i)\n",
    "\t\tY = getattr(self,'Y'+id_i)\n",
    "\n",
    "\t\ttrain_x, val_x ,train_y, val_y  = train_test_split(X,Y,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=seed)\n",
    "\t\t# Split train into train and test\n",
    "\t\ttrain_x, test_x, train_y, test_y = train_test_split(train_x,train_y,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.25,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=seed)\n",
    "\n",
    "\t\tsetattr(self,'X_train'+id_i,train_x)\n",
    "\t\tsetattr(self,'X_test'+id_i,test_x )\n",
    "\t\tsetattr(self,'X_val'+id_i,val_x)\n",
    "\t\tsetattr(self,'Y_train'+id_i,train_y )\n",
    "\t\tsetattr(self,'Y_test'+id_i,test_y )\n",
    "\t\tsetattr(self,'Y_val'+id_i,val_y )\n",
    "\n",
    "\tdef train_and_val(self,model,X,Y,X_val,Y_val):\n",
    "\t\t# Train and validate the model\n",
    "\t\tmodel.fit(X,Y,\n",
    "\t\t\t\t  validation_data=(X_val,Y_val),\n",
    "\t\t\t\t  batch_size=200,\n",
    "\t\t\t\t  epochs=3000,\n",
    "\t\t\t\t  shuffle=False)\n",
    "\n",
    "\tdef train_procedure(self,id_i):\n",
    "\t\tmodel  = getattr(self,'model'+id_i)\n",
    "\t\ttrain  = getattr(self,'X_train'+id_i)\n",
    "\t\tval    = getattr(self,'X_val'+id_i)\n",
    "\n",
    "\t\tself.train_and_val(model,train,\n",
    "\t\t\t\t\t\t   train,\n",
    "\t\t\t\t\t\t   val,\n",
    "\t\t\t\t\t\t   val)\n",
    "\n",
    "\tdef score(self,model,id,X_test,Y_test):\n",
    "\t\tscores = model.evaluate(X_test,Y_test)\n",
    "\t\t# evaluate the model\n",
    "\t\tprint(\"error %f\" %scores[0])\n",
    "\n",
    "\tdef model_out(self):\n",
    "\t\tprint(\"\")\n",
    "\n",
    "\tdef total_out(self):\n",
    "\t\terror_1 = list()\n",
    "\t\terror_2 = list()\n",
    "\t\terror_3 = list()\n",
    "\n",
    "\t\terror = list()\n",
    "\t\tself.out = list()\n",
    "\n",
    "\t\tfor x in range(0,len(self.Y[:,0])):\n",
    "\t\t\ty = x+1\n",
    "\t\t\tscores = self.model1.evaluate(self.X[x:y],self.X[x:y])\n",
    "\t\t\terror_1.append(scores[0])\n",
    "\n",
    "\t\tfor x in range(0,len(self.Y[:,0])):\n",
    "\t\t\ty = x+1\n",
    "\t\t\tscores = self.model2.evaluate(self.X[x:y],self.X[x:y])\n",
    "\t\t\terror_2.append(scores[0])\n",
    "\n",
    "\t\tfor x in range(0,len(self.Y[:,0])):\n",
    "\t\t\ty = x+1\n",
    "\t\t\tscores = self.model3.evaluate(self.X[x:y],self.X[x:y])\n",
    "\t\t\terror_3.append(scores[0])\n",
    "\n",
    "\t\terror.append(error_1)\n",
    "\t\terror.append(error_2)\n",
    "\t\terror.append(error_3)\n",
    "\t\terror = pd.DataFrame(error)\n",
    "\t\terror = error.transpose()\n",
    "\t\terror = error.values\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(error_2)\n",
    "\t\tplt.show()\n",
    "\t\tfor i in range(0,len(error[:,0])):\n",
    "\t\t\tout_i = np.argmin(error[i])\n",
    "\t\t\tself.out.append(out_i)\n",
    "\n",
    "\t\tself.out = np.add(self.out,1)\n",
    "\t\t#print(len(self.Y_test1[:,0]),len(self.Y_test2[:,0]),len(self.Y_test3[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
